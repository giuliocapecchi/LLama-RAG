{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliocapecchi/LM-project/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMdgKoa5LF9w"
      },
      "source": [
        "# PDF Preprocessing and Documents Creations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OV1QFHWLvIw"
      },
      "source": [
        "Nowadays one of the challenging aspects of preprocessing scientific documents, for the implementation of a RAG (*Retrieval-Augmented Generation*) system, is the accurate **extraction of mathematical formulas**. Considering working with files containing this type of information, we faced the need to adopt a solution that would allow us to achieve results as good and reliable as possible using free tools and resources.\n",
        "\n",
        "In addition to this, for completeness we report other main relevant tasks, regarding the processing of files in order to implement a RAG system, we tried to manage:\n",
        "*   **handle complex structures** mantaning contextual meaning;\n",
        "*   **text cleaning** removing irrilevant elements;\n",
        "*   **chunk size balancing** accordingly with the specific context limit for the chosen LLM;\n",
        "*   **text splitting** avoiding breaking meaningful connections in sentences.\n",
        "\n",
        "For this purpose we propose an hybrid approach that combines multiple solutions. In particular we built a pipeline that integrates functionalities of two solutions:\n",
        "*   `Unstructure`, an open-source tool efficient in transforming unstructured data into structured outputs.\n",
        "    *   https://github.com/Unstructured-IO\n",
        "*   `Nougat`, an advanced Transformer-based OCR model that simplifies the process of converting complex scientific contents into a common and machine-readable format.\n",
        "    *   https://github.com/facebookresearch/nougat\n",
        "\n",
        "The tasks we have addressed to these tools are respectively:\n",
        "1.   Parse the PDF file obtaning **elements characterized by type and content** through `Unstructure partition function`. This allowed us to:\n",
        "    *   clean the extracted contents **filering out irrelevant or meaningless types** of elements;\n",
        "    *   identify page breaks, permetting to implement a **page content-aware solution**;\n",
        "    *   **locate the formulas**, saving indexes of pages containing them;\n",
        "    *   translate elements into **a dataframe, with columns Type and Value**, for easier handling.\n",
        "\n",
        "2.   Extract formulas in more accurate and replicable Latex format thanks to `Nougat`. This was done according to the following logic:\n",
        "    *   **only pages in** which we know **there is at least one formula are processed**, allowing us to save resources and time by avoiding analyzing the pages of the file in which no formulas appear;\n",
        "    *   the selected pages are converted into images, analyzed using a **Transformer-based OCR model**, and returned as text formatted in LaTeX;\n",
        "    *   the model's output is analyzed to identify **Latex patterns matching formulas**, which are then extracted.\n",
        "\n",
        "This approach was necessary because, despite employing a high-resolution element identification strategy in Unstructured partitioning, the formulas often turned out incorrect and failed to meet our expectations. Therefore, we retained the use of `Unstructured` for its ability to implement an **elements and page-aware solution** while integrating `Nougat` in a specialized manner **for extracting mathematical formulas**. This combination allowed us to achieve significantly improved and optimal results.\n",
        "\n",
        "Continuing with the numbering used above, the final processing steps are:\n",
        "3.   **Replace the more precise formulas in the dataframe** where all the elements are stored. By doing this we faced the problem of mismatch between expected and extracted formulas for a single page:\n",
        "    *   if no formulas extracted by `Nougat` we keep the formulas as extracted by `Unstructure`;\n",
        "    *   else, if the number of extracted formulas differs from the expected one, we give priority to the formulas extracted by `Nougat`, due to more precision and accuracy, discarding the ones extracted by `Unstructure`.\n",
        "4.   **Page-based Chunks creation**. This was done according to the following logic:\n",
        "    *   setting a **threshold as the maximum number of tokens** that can be included into a single chunk;\n",
        "    *   creating chunks by **concatenating page-by-page content** to preserve contextual integrity as much as possible;\n",
        "    *   if the content of a single page exceeds the threshold, we concatenate **elements one by one until the content remains under the limit**. This approach is a tradeoff between the balancing of chunk sizes and the coherence of contextual structures.\n",
        "\n",
        "Let's dive into the implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmLEt9LCLpLn"
      },
      "source": [
        "Installing required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3UrpIY5sLgF"
      },
      "outputs": [],
      "source": [
        "# %pip install -qqq torch gdown huggingface_hub python-dotenv pymupdf python-Levenshtein nltk chromadb tqdm unidecode gradio bitsandbytes seaborn unstructured[pdf] latex2mathml --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QRv6r2rWNRw"
      },
      "source": [
        "We start off by downloading the 'assets' folder, that contains images rendered in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-lBuhHdWNRw",
        "outputId": "9103e8d2-64d9-451d-a0fc-c82742a44233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assets folder 'assets' already exists. Skipping download and extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "zip_url = \"https://drive.google.com/uc?export=download&id=1iira8TFvy7Nix3NwusLMLqgTCmjy76Ru\"\n",
        "zip_file_name = \"assets.zip\"\n",
        "output_dir = \"assets\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    print(\"Downloading assets...\")\n",
        "    response = requests.get(zip_url, stream=True)\n",
        "    with open(zip_file_name, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download completed.\")\n",
        "\n",
        "    # Estrai il file zip\n",
        "    print(\"Unzipping assets...\")\n",
        "    with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    print(f\"Assets unzipped to '{output_dir}'.\")\n",
        "\n",
        "    os.remove(zip_file_name)\n",
        "    print(\"Cleanup completed.\")\n",
        "else:\n",
        "    print(f\"Assets folder '{output_dir}' already exists. Skipping download and extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FPsNo-aWNRy"
      },
      "source": [
        "### Installing Additional Required Libraries\n",
        "\n",
        "On Colab, the installations above are sufficient. However, on **Windows** and **macOS**, additional steps are required to install libraries necessary for **Unstructured** and **Nougat**:\n",
        "\n",
        "- **Poppler**: a PDF rendering library.  \n",
        "- **Tesseract**: an open-source OCR engine.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Steps for Windows**\n",
        "The easiest way to install the required libraries on Windows is via Conda (assuming Conda is installed):\n",
        "\n",
        "1. **Install the required packages**:\n",
        "    ```bash\n",
        "    %conda install -c conda-forge poppler tesseract -y\n",
        "    %pip install -q python-magic-bin==0.4.14\n",
        "    ```\n",
        "\n",
        "2. **Add Tesseract to the system PATH**:\n",
        "    Use the following Python code to set up the `TESSDATA_PREFIX` environment variable:\n",
        "\n",
        "    ```python\n",
        "    import os, sys\n",
        "\n",
        "    # Get the path of the current Conda environment\n",
        "    conda_env_path = os.path.dirname(sys.executable)\n",
        "    tessdata_path = os.path.join(conda_env_path, \"share\", \"tessdata\") # the necessary files should be here\n",
        "\n",
        "    # Set the TESSDATA_PREFIX environment variable\n",
        "    os.environ[\"TESSDATA_PREFIX\"] = os.path.join(conda_env_path, \"share\")\n",
        "    print(\"TESSDATA_PREFIX:\", os.environ[\"TESSDATA_PREFIX\"])\n",
        "\n",
        "    # Verify the tessdata directory\n",
        "    if os.path.exists(tessdata_path):\n",
        "        print(f\"Tessdata directory found: {tessdata_path}\")\n",
        "    else:\n",
        "        print(f\"Tessdata directory not found: {tessdata_path}\")\n",
        "    ```\n",
        "\n",
        "#### **Steps for macOS**\n",
        "On macOS, installation is simpler. Use the following command to install the required libraries:\n",
        "\n",
        "1. **Install the required packages**:\n",
        "    ```bash\n",
        "    brew install poppler tesseract\n",
        "    ```\n",
        "\n",
        "2. **Verify the installation**:\n",
        "    Check if Tesseract is properly installed by running:\n",
        "    ```bash\n",
        "    tesseract --version\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "Still, this approach serves as an enhancement to the RAG pipeline, enabling more accurate extraction from PDFs, particularly for elements like mathematical formulas in scientific papers. However, it is not mandatory; if the PDF contains mostly plain text, a simpler chunking method may be sufficient and more straightforward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L7OPE5AIZM8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "print(nltk.__version__)  # must be 3.9.1\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo9QzZJzSLKe"
      },
      "source": [
        "Let's download the selected PDF for this analysis. You can choose any PDF you prefer, but keep in mind that your evaluation phase will have to differ.\n",
        "\n",
        "> **Note:** To speed up execution, we’ll also download the `elements.pkl` file to bypass some computationally expensive processing (which would otherwise take approximately *20/30 minutes* for ~300 pages)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9hQwckZHXHO"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "\n",
        "PDF_NAME = \"Information Retrieval Slides.pdf\"\n",
        "if os.path.exists(PDF_NAME):\n",
        "    print(f\"File {PDF_NAME} already exists.\")\n",
        "else:\n",
        "    # save locally from https://drive.google.com/file/d/1xUA6_ZBJzWGF7kWpM1YZTK3R1siYg1qY/view?usp=drive_link\n",
        "    gdown.download(id=\"1xUA6_ZBJzWGF7kWpM1YZTK3R1siYg1qY\", output=PDF_NAME, quiet=False)\n",
        "\n",
        "    # download of pickled elements to speed up the execution\n",
        "    gdown.download(id=\"17eXYgmiTL9-f9F5vIT5j7QffHIu_QwuN\", output=\"elements.pkl\", quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_HRybt8S-Xp"
      },
      "source": [
        "## PDF Partitioning with Unstructured\n",
        "\n",
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "    .center {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "    }\n",
        "    .center img {\n",
        "        width: 50%;\n",
        "   }\n",
        "</style>\n",
        "\n",
        "<div class=\"center\">\n",
        "    <img src=\"https://github.com/giuliocapecchi/LM-project/blob/main/assets/unstructured.png?raw=1\" alt=\"unstructured\">\n",
        "</div>\n",
        "\n",
        "\n",
        "Processes the PDF using the `partition` function to extract structured elements.\n",
        "\n",
        "We decide to apply the **high-resolution strategy** because is highly sensitive, it is recommended if you want obtain precise classifications for document elements. Adopting this strategy we are able to use the document layout to gain additional information about document elements, in particular we are able to recognize:\n",
        "- **Titles** and **Text** (we use them as it is)\n",
        "- **Formulas** (we further process them)\n",
        "- **Images** (will be removed)\n",
        "- **Page Breaks** (fundamental to handle formulas and create chuncks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpVjk7JfIoTh"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.auto import partition\n",
        "from tqdm import tqdm\n",
        "import fitz  # PyMuPDF\n",
        "import pickle\n",
        "\n",
        "\n",
        "load_elements_from_pickle = True\n",
        "elements = []\n",
        "\n",
        "if load_elements_from_pickle:\n",
        "    # Loading the pickled elements.\n",
        "    with open('elements.pkl', 'rb') as f:\n",
        "        elements = pickle.load(f)\n",
        "else: # Process the PDF file\n",
        "    doc = fitz.open(PDF_NAME)  # open the PDF file\n",
        "    total_pages = len(doc)\n",
        "\n",
        "    for page_number in tqdm(range(total_pages), desc=\"Processing PDF pages\"):\n",
        "        # Crea un nuovo documento con una sola pagina\n",
        "        temp_doc = fitz.open()  # Documento vuoto\n",
        "        temp_doc.insert_pdf(doc, from_page=page_number, to_page=page_number)\n",
        "\n",
        "        # Salva la singola pagina in un file temporaneo\n",
        "        temp_page_file = f\"temp_page_{page_number}.pdf\"\n",
        "        temp_doc.save(temp_page_file)\n",
        "        temp_doc.close()\n",
        "\n",
        "        # Applica `partition` sulla singola pagina\n",
        "        page_elements = partition(\n",
        "            filename=temp_page_file,\n",
        "            strategy=\"hi_res\",\n",
        "            skip_infer_table_types=[],\n",
        "            include_page_breaks=True,\n",
        "        )\n",
        "        elements.extend(page_elements)  # Aggiungi gli elementi alla lista totale\n",
        "        # delete the temporary file\n",
        "        os.remove(temp_page_file)\n",
        "\n",
        "    doc.close()  # Chiudi il documento principale\n",
        "\n",
        "# Stampa il numero totale di elementi\n",
        "print(\"Number of recognized elements: \" + str(len(elements)) + \" in file \" + PDF_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CITylvMsd1Mf"
      },
      "source": [
        "Storing the elements in a pickle file.\n",
        "Snippet of code executed if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_6QJqgxj4ou"
      },
      "outputs": [],
      "source": [
        "# to uncomment, now is commented cause we are downloading the pickled elements\n",
        "with open('elements.pkl', 'wb') as f:\n",
        "    pickle.dump(elements, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfWUBRLfjAK"
      },
      "source": [
        "## Filtering and Analyzing Extracted Elements\n",
        "\n",
        "Here we build a **dataframe**, creating pairs of **Type** and **Value**, using the elements extracted from the PDF file.\n",
        "\n",
        "Then we print all the different element's Type recognized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYMr613sfcqH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_elements = pd.DataFrame( [[type(elem), elem.text] for elem in elements], columns=['Type', 'Value'])\n",
        "\n",
        "# Print all the unique types of elements\n",
        "print(df_elements['Type'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuoY63qTQy4"
      },
      "source": [
        "Now we clean the dataframe filtering out all the useless elements:\n",
        "- **Images** (for them we have a text extracted with OCR technique, but not very useful for the language model due to a loss of meaning)\n",
        "- **Table** (similarly to the images they lost some meaning as they are extracted)\n",
        "- **Footers** and **Headers**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6iG38ZWwv7L"
      },
      "outputs": [],
      "source": [
        "from unstructured.documents.elements import Image, Table, Footer, Header, PageBreak, Formula\n",
        "\n",
        "# print df number of rows\n",
        "print(\"Number of actual elements: \" + str(len(df_elements)))\n",
        "\n",
        "# drop all the rows with unuseful Types\n",
        "df_elements = df_elements[df_elements['Type'] != Image]\n",
        "df_elements = df_elements[df_elements['Type'] != Table]\n",
        "df_elements = df_elements[df_elements['Type'] != Footer]\n",
        "df_elements = df_elements[df_elements['Type'] != Header]\n",
        "df_elements = df_elements.drop(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')].index)\n",
        "\n",
        "# print df number of rows\n",
        "print(\"Number of elements after filtering: \" + str(len(df_elements)))\n",
        "\n",
        "# reassign the dataframe index\n",
        "df_elements = df_elements.reset_index(drop=True)\n",
        "\n",
        "# print all the different possible Type in the dataframe\n",
        "print(\"Remaining Types left in the dataframe:\\n\",df_elements['Type'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aswk1lwSlziI"
      },
      "source": [
        "Check if there are some empty-valued elements, that are not Page Breaks, in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vBne_I70vij"
      },
      "outputs": [],
      "source": [
        "# print rows of df_elements with bith Type != PageBreak and Value empty\n",
        "print(len(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')]))\n",
        "\n",
        "if len(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')]) > 0:\n",
        "  df_elements = df_elements.drop(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrkLBolTsQv"
      },
      "source": [
        "## Filtering Pages Containing Formulas in a DataFrame\n",
        "\n",
        "This code processes a DataFrame (`df_elements`) containing elements from a document (e.g., text, formulas, and page breaks) to identify and remove all rows belonging to pages that contain at least one formula.\n",
        "\n",
        "- `pages_with_formulas_indexes`: A `set` to store the indices of pages that contain at least one `Formula`.\n",
        "- `index`: Keeps track of the current page number (starts at 0).\n",
        "- `page_indices`: A list to map each row in the DataFrame to its corresponding page number.\n",
        "\n",
        "\n",
        "In the end, we filter out rows whose `PageIndex` matches any value in `pages_with_formulas_indexes`. The `PageIndex` column is dropped after filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rq_yIt4WNR5"
      },
      "outputs": [],
      "source": [
        "# Initial number of PageBreak elements\n",
        "print(\"Number of PageBreak elements: \" + str(len(df_elements[df_elements['Type'] == PageBreak])))\n",
        "\n",
        "\n",
        "# list of page indexes with at least one formula\n",
        "pages_with_formulas_indexes = set()\n",
        "index = 0\n",
        "page_indices = [] # to map each row to its page index\n",
        "\n",
        "for i in range(len(df_elements)):\n",
        "    # when a PageBreak is found, increment the page-index\n",
        "    if df_elements.iloc[i]['Type'] == PageBreak:\n",
        "        index += 1\n",
        "    page_indices.append(index)\n",
        "    # if you find a formula, add the page index to the set\n",
        "    if df_elements.iloc[i]['Type'] == Formula:\n",
        "        pages_with_formulas_indexes.add(index)\n",
        "\n",
        "# add the PageIndex column to the DataFrame\n",
        "df_elements['PageIndex'] = page_indices\n",
        "\n",
        "# filter out the rows with formulas\n",
        "df_elements_cleaned = df_elements[~df_elements['PageIndex'].isin(pages_with_formulas_indexes)].drop(columns=['PageIndex']).reset_index(drop=True)\n",
        "\n",
        "# Final number of PageBreak elements\n",
        "print(\"Number of PageBreak elements: \" + str(len(df_elements_cleaned[df_elements_cleaned['Type'] == PageBreak])))\n",
        "print(\"Pages with formulas indexes:\", sorted(pages_with_formulas_indexes))\n",
        "print(\"Number of pages with at least one formula:\", len(pages_with_formulas_indexes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPbgAD5jWNR5"
      },
      "outputs": [],
      "source": [
        "print(\"Number of elements before filtering:\", len(df_elements))\n",
        "print(\"Number of elements after filtering:\", len(df_elements_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZqXuKB-WNR5"
      },
      "source": [
        "The Unstructured pipeline can make some mistakes in extracting text from the pdf. This code replaces uncommon Unicode characters with ASCII equivalents and removes any remaining non-ASCII characters. We defined a dictionary for replacing specific Unicode characters (e.g., `ﬃ → ffi`, `⊺ → T`), more can be added if necessary. We use `str.maketrans()` for fast replacements. The computed values are stored back in the `Value` column of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9wSR0CQWNR5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "unicode_replacements = {\n",
        "    \"ﬃ\": \"ffi\",\n",
        "    \"ﬀ\": \"ff\",\n",
        "    \"ﬁ\": \"fi\",\n",
        "    \"⊺\": \"T\",\n",
        "    \"•\": \" \",\n",
        "    \"—\": \"-\",\n",
        "    \"«\": \" \",\n",
        "    \"»\": \" \",\n",
        "    \"”\": '\"',\n",
        "    \"“\": '\"',\n",
        "    \"‘\": \"'\",\n",
        "    \"’\": \"'\",\n",
        "    \"ﬄ\": \"ffl\",\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "# Create a translation table for faster replacement\n",
        "translation_table = str.maketrans(unicode_replacements)\n",
        "\n",
        "def find_non_ascii_characters(text):\n",
        "    # Replace all characters using the translation table\n",
        "    text = text.translate(translation_table)\n",
        "    # Optional: Remove any remaining non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]', '', text)  # Removes all remaining non-ASCII characters\n",
        "    return text\n",
        "\n",
        "# Remove non-ASCII characters from each element's text\n",
        "for i in range(len(df_elements_cleaned)):\n",
        "    df_elements_cleaned.loc[i, 'Value'] = find_non_ascii_characters(df_elements_cleaned.loc[i, 'Value'])\n",
        "\n",
        "\n",
        "df_elements = df_elements_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0oMEuCOT0H2"
      },
      "source": [
        "## Nougat: Neural Optical Understanding for Academic Documents\n",
        "\n",
        "\n",
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "    .center {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "    }\n",
        "    .center img {\n",
        "        width: 30%;\n",
        "    }\n",
        "</style>\n",
        "<div class=\"center\">\n",
        "    <img src=\"https://github.com/giuliocapecchi/LM-project/blob/main/assets/meta-emblem.png?raw=1\" alt=\"meta-emblem\">\n",
        "</div>\n",
        "\n",
        "\n",
        "[**Nougat**](https://github.com/facebookresearch/nougat?tab=readme-ov-file) is an academic document PDF parser designed to extract and understand LaTeX math and tables from PDFs. Developed by **Facebook Research**, it converts academic documents into structured formats (Mathpix Markdown, *.MMD*).\n",
        "\n",
        "Key Features:\n",
        "- Extracts and processes LaTeX math, tables, and text.\n",
        "- Converts PDFs to Markdown (.mmd) format.\n",
        "- Supports GPU for faster processing.\n",
        "\n",
        "Nougat is optimized for scientific papers, especially those found on platforms like arXiv, and works best with English papers.\n",
        "\n",
        "The idea now is to employ it as a *bullet-proof* approach for extracting text difficult pages, like the ones with mathemathical formulas (which indexes were saved above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivo3rJ4OWNR6"
      },
      "source": [
        "> As a side note, unfortunately Nougat requires `transformers==4.38.2` or below, which is uncompatible with `sentence-transformers`, since it requires `transformers<5.0.0,>=4.41.0`. A solution to this is to keep them separated in two different virutal environments. For the sake of semplicity, in the notebook we will simply upgrade `transformers` later on in order to utilize `sentence-transformers` without any problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmq5mVmfWNR6"
      },
      "outputs": [],
      "source": [
        "# install some pre-requisites for Nougat\n",
        "%pip install -qqq -U albumentations\n",
        "%pip -qqq install transformers==4.38.2 --progress-bar off\n",
        "\n",
        "# install Nougat\n",
        "%pip -qqq install nougat-ocr  --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJEnWwFdWNR6"
      },
      "source": [
        "Now we can simply invoke Nougat, by calling the corresponding CLI command. More information about it can be found on its [README page](https://github.com/facebookresearch/nougat?tab=readme-ov-file). Here it is called by simply passing:\n",
        "- the path of the PDF file in analysis\n",
        "- `-o`, the output folder\n",
        "- `--pages`, to specify only pages containing mathemathical formulas\n",
        "- `-m 0.1.0-base`, to utilize the base model (default to `0.1.0-small` but didn't produce acceptable results)\n",
        "- `--no-skipping`, to avoid using failure detection heuristic\n",
        "- `--recompute`, to recompute already computed PDF, discarding previous predictions.\n",
        "> Requires ~1.3 minutes for 22 pages with formulas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewz8syhoWNR6"
      },
      "outputs": [],
      "source": [
        "# Nougat is 1-indexed, so we need to increment the page indexes by 1\n",
        "nougat_formulas_indexes = [index + 1 for index in pages_with_formulas_indexes]\n",
        "\n",
        "nougat_formulas_indexes_str = \",\".join(map(str, sorted(nougat_formulas_indexes)))\n",
        "print(nougat_formulas_indexes_str)\n",
        "\n",
        "command = f'nougat \"{PDF_NAME}\" -o ./nougat-output --pages {nougat_formulas_indexes_str} -m 0.1.0-base --no-skipping --recompute'\n",
        "\n",
        "print(command)\n",
        "\n",
        "if not os.path.exists(f\"nougat-output/{PDF_NAME.replace('.pdf', '.mmd')}\"):\n",
        "    os.system(command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs4Ea2tIWNR7"
      },
      "source": [
        "The output can be found in the `nougat-output` folder, as an MMD file. Since of course the model can fail to identify some sentences, we also perform some preprocessing of the text, to ensure it will be sanitized for the next phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhsgB3aWNR7"
      },
      "outputs": [],
      "source": [
        "def remove_unmatched_left_tags(line):\n",
        "        if \"\\\\left|\" in line and \"\\\\right|\" not in line:\n",
        "            line = line.replace(\"\\\\left|\", \"\")\n",
        "        return line\n",
        "\n",
        "def preprocess_content(content):\n",
        "    content = content.replace(\"**\", \"\").replace(\"##\", \"\")  # Some markdown is left sometimes\n",
        "    content = content.replace(\"{(}\", \"(\").replace(\"{)}\", \")\") # replace all '{(}' and '{)}' with '(' and ')'\n",
        "    content = content.replace(\"{[}\", \"(\").replace(\"{]}\", \")\") # replace all '{[}' and '{]}' with '(' and ')'\n",
        "    content = re.sub(r'\\$(.*?)\\$', r'\\\\(\\1\\\\)', content) # replace all '$' with '\\(\\)' (inline formulas)\n",
        "    content = re.sub(r'\\\\text\\{(.*?)\\}', r'\\1', content) # replace all '\\text' with ''\n",
        "    content = re.sub(r'_(.*?)_', r'\\1', content) # replace words inside '_word_' with the same word -> e.g. _Proof:_ -> Proof:\n",
        "    content = re.sub(r'\\\\includegraphics\\[width=.*?pt\\]', '', content) # replace all \\includegraphics[width=....pt] with '', independently of the width\n",
        "    content = \"\\n\".join(remove_unmatched_left_tags(line) for line in content.splitlines()) # remove all \\left|\\ tags that don't have a \\right|\\ tag in the same line (they are not closed)\n",
        "    content = content.replace('\\\\\\\\', '\\\\') # substitute '\\\\' with '\\', since it's not necessary to escape the backslash\n",
        "    content = content.strip()\n",
        "    return content\n",
        "\n",
        "\n",
        "with open(f'nougat-output/{PDF_NAME.replace(\".pdf\",\".mmd\")}', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "print(content[:200], \" ...\\n\\n..........................................\")\n",
        "\n",
        "processed_content = preprocess_content(content)\n",
        "\n",
        "\n",
        "with open(f'nougat-output/{PDF_NAME.replace(\".pdf\",\".mmd\")}', 'w') as f:\n",
        "    f.write(processed_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Gp1sZRWNR7"
      },
      "source": [
        "Finally the produced text with formulas is re-added to the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1-7NgCVMY9"
      },
      "source": [
        "# Creating the chunks\n",
        "\n",
        "Let's start by cleaning up the `df_elements` DataFrame by removing consecutive duplicate rows (keeping the first occurence). This can happen because... TODO boh ? non dovrebbe in realtà quindi non ho idea del motivo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBJg2MZ3QH1t"
      },
      "outputs": [],
      "source": [
        "print(\"Number of elements before removing adjacent duplicates: \" + str(len(df_elements)))\n",
        "\n",
        "# drop adjacent duplicates in the df_elements mantaining the first one\n",
        "df_elements = df_elements.loc[df_elements.shift().ne(df_elements).any(axis=1)]\n",
        "\n",
        "# reset index\n",
        "df_elements = df_elements.reset_index(drop=True)\n",
        "\n",
        "print(\"Number of elements after removing adjacent duplicates: \" + str(len(df_elements)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_33-bMTulyW"
      },
      "outputs": [],
      "source": [
        "print(\"Final number of pages (given by the number of PageBreak(s)):\",len(df_elements[df_elements['Type'] == PageBreak]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vMk0-oV-_1B"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def count_tokens(text):\n",
        "  return len(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v598nbR51juB"
      },
      "source": [
        "Here we compute some statistics regarding the number of tokens in the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpV8q_skMHGl"
      },
      "outputs": [],
      "source": [
        "# compute the max, the min and the average number of tokens in a page by concatenating elements of the dataframe between consecutive page breaks\n",
        "\n",
        "num_token_per_page = []\n",
        "\n",
        "current_page = \"\"\n",
        "current_page_tokens = 0\n",
        "\n",
        "for i in range(len(df_elements)):\n",
        "  if df_elements.iloc[i]['Type'] != PageBreak:\n",
        "    current_page += df_elements.iloc[i]['Value']\n",
        "    current_page_tokens = count_tokens(current_page)\n",
        "  else:\n",
        "    num_token_per_page.append(current_page_tokens)\n",
        "    current_page = \"\"\n",
        "    current_page_tokens = 0\n",
        "\n",
        "print(\"Max tokens: \" + str(max(num_token_per_page)))\n",
        "print(\"Min tokens: \" + str(min(num_token_per_page)))\n",
        "print(\"Avg tokens: \" + str(sum(num_token_per_page)/len(num_token_per_page)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SttzqQSMhxG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mean_tokens = np.mean(num_token_per_page)\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.hist(num_token_per_page, bins=10, edgecolor='black', alpha=0.5)\n",
        "plt.axvline(mean_tokens, color='red', linestyle='dashed', linewidth=1, label=f'Mean = {mean_tokens:.2f}')\n",
        "plt.title('Distribution of Number of Tokens per Page')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE2y3MZgVuIa"
      },
      "source": [
        "# Splitting Content into Tokenized Chunks\n",
        "Here we divide the processed elements in `df_elements` into smaller text files, ensuring each file stays within a token limit.\n",
        "\n",
        "**Chunk Creation:**\n",
        "- Processes elements in `df_elements`:\n",
        "  - Concatenates values until the token count exceeds `doc_tokens_threshold`.\n",
        "  - Writes each chunk to a separate text file (`docX.txt`).\n",
        "  - Tracks the token count of each chunk in `docs_len`.\n",
        "- Handles `PageBreak` to reset and manage document boundaries.\n",
        "\n",
        "\n",
        ">NOTE: `doc_tokens_threshold` can be adjusted to change the maximum token count per file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6CxZzdOEqnJ"
      },
      "outputs": [],
      "source": [
        "# Create a directory to store the chunks\n",
        "if not os.path.exists('chunks'):\n",
        "    os.makedirs('chunks')\n",
        "else:\n",
        "    for file in os.listdir('chunks'):\n",
        "        os.remove(os.path.join('chunks', file))\n",
        "\n",
        "block = \"\"\n",
        "document = \"\"\n",
        "doc_index = 0\n",
        "doc_tokens_threshold = 300\n",
        "docs_len = []\n",
        "overlap_tokens = 30  # Numero di token di overlapping da trasferire\n",
        "\n",
        "# Iterate over the elements of the dataframe\n",
        "for i in range(len(df_elements)):\n",
        "    element_type = df_elements.at[i, 'Type']\n",
        "    next_element = df_elements.at[i, 'Value'] + \" \"\n",
        "\n",
        "    # Verifica se l'elemento è una formula\n",
        "    if element_type == 'Formula':\n",
        "        # Aggiungiamo la formula direttamente al blocco\n",
        "        block += next_element\n",
        "        continue  # Saltiamo la logica di split per le formule\n",
        "\n",
        "    if element_type != PageBreak:\n",
        "        # Verifica se il blocco corrente supera la soglia\n",
        "        if count_tokens(block) + count_tokens(next_element) > doc_tokens_threshold:\n",
        "            # Se il blocco ha superato il limite, salviamo il blocco e creiamo il nuovo file\n",
        "            with open(f'chunks/document_{doc_index}.txt', 'w') as f:\n",
        "                f.write(block)\n",
        "            docs_len.append(count_tokens(block))\n",
        "            doc_index += 1\n",
        "\n",
        "            # Creiamo un overlap con l'ultima parte del blocco precedente\n",
        "            overlap = block.split()[-overlap_tokens:]\n",
        "            block = \" \".join(overlap) + \" \" + next_element\n",
        "        else:\n",
        "            # Altrimenti aggiungiamo l'elemento al blocco\n",
        "            block += next_element\n",
        "    else:  # Se l'elemento è un PageBreak\n",
        "        if count_tokens(document) + count_tokens(block) > doc_tokens_threshold:\n",
        "            with open(f'chunks/document_{doc_index}.txt', 'w') as f:\n",
        "                f.write(document)\n",
        "            docs_len.append(count_tokens(document))\n",
        "            doc_index += 1\n",
        "            document = \"\"\n",
        "\n",
        "        document += block\n",
        "        block = \"\"\n",
        "\n",
        "# Scriviamo l'ultimo blocco se necessario\n",
        "if block:\n",
        "    with open(f'chunks/document_{doc_index}.txt', 'w') as f:\n",
        "        f.write(block)\n",
        "    docs_len.append(count_tokens(block))\n",
        "\n",
        "if document:\n",
        "    with open(f'chunks/document_{doc_index}.txt', 'w') as f:\n",
        "        f.write(document)\n",
        "    docs_len.append(count_tokens(document))\n",
        "\n",
        "print(\"Number of chunks obtained: \" + str(doc_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAlhDyZFWNSC"
      },
      "outputs": [],
      "source": [
        "# Now we chunk also the formulas obtained by Nougat, which is inside 'processed_content'\n",
        "# the approach is almost the same. Create chunks of 'doc_tokens_threshold' with 'overlap_tokens' tokens of overlapping\n",
        "# we must also not break formulas in half. Formulas are between '\\[' and '\\]', and '\\(' and '\\)'.\n",
        "# the index starts from 'doc_index' + 1 and is stored in the previous directory 'chunks'\n",
        "\n",
        "doc_tokens_threshold = 150\n",
        "\n",
        "# Helper function to count tokens (use your existing count_tokens function)\n",
        "def count_tokens(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Function to detect formulas inline and display\n",
        "def detect_and_split_formulas(text):\n",
        "    # This function detects and preserves formulas, ensuring they aren't split\n",
        "    # Inline formulas ( \\( ... \\) ) and display formulas ( \\[ ... \\] )\n",
        "    text = re.sub(r'\\\\\\((.*?)\\\\\\)', r'\\\\\\(\\1\\\\\\)', text)  # Keep inline formulas intact\n",
        "    text = re.sub(r'\\\\\\[(.*?)\\\\\\]', r'\\\\\\[\\1\\\\\\]', text)  # Keep display formulas intact\n",
        "    return text\n",
        "\n",
        "# Iterate over the processed content\n",
        "processed_content = detect_and_split_formulas(processed_content)\n",
        "\n",
        "# Split content into words or tokens\n",
        "words = processed_content.split()\n",
        "\n",
        "block = \"\"\n",
        "for i in range(len(words)):\n",
        "    next_word = words[i] + \" \"\n",
        "\n",
        "    # Check if the block exceeds the token threshold\n",
        "    if count_tokens(block) + count_tokens(next_word) > doc_tokens_threshold:\n",
        "        # If the block exceeds the limit, save the block and create a new file\n",
        "        with open(f'chunks/document_{doc_index + 1}.txt', 'w') as f:\n",
        "            f.write(block)\n",
        "        docs_len.append(count_tokens(block))\n",
        "        doc_index += 1\n",
        "\n",
        "        # Create overlap with the last part of the previous block\n",
        "        overlap = block.split()[-overlap_tokens:]\n",
        "        block = \" \".join(overlap) + \" \" + next_word\n",
        "    else:\n",
        "        # Otherwise, add the word to the block\n",
        "        block += next_word\n",
        "\n",
        "# Write the last block if needed\n",
        "if block:\n",
        "    with open(f'chunks/document_{doc_index + 1}.txt', 'w') as f:\n",
        "        f.write(block)\n",
        "    docs_len.append(count_tokens(block))\n",
        "\n",
        "print(\"Number of chunks obtained: \" + str(doc_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwVWd3HhzOov"
      },
      "outputs": [],
      "source": [
        "# print the max, the min and the avg of values in docs_len\n",
        "print(\"Number of documents: \",len(docs_len))\n",
        "print(\"Maximum number of tokens in a document: \",max(docs_len))\n",
        "print(\"Minimum number of tokens in a document: \",min(docs_len))\n",
        "print(\"Average number of tokens in a document: \",sum(docs_len)/len(docs_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2aDAKvnOByr"
      },
      "outputs": [],
      "source": [
        "# plot the distribution of number of tokens per document showing the mean\n",
        "mean_tokens = np.mean(docs_len)\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.hist(docs_len, bins=10, edgecolor='black', alpha=0.5)\n",
        "plt.axvline(mean_tokens, color='red', linestyle='dashed', linewidth=1, label=f'Mean = {mean_tokens:.2f}')\n",
        "plt.title('Distribution of Number of Tokens per Document')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5j3ay1zTLO5"
      },
      "source": [
        "# RAG Pipeline on PDFs with Limited GPU Requirements\n",
        "\n",
        "In this project, we aim to build a **Retrieval-Augmented Generation (RAG)** pipeline optimized for **limited GPU environments**. The goal is to combine the power of *dense retrieval-based methods* with the flexibility of generative models, while ensuring the system remains efficient enough to run on medium-tier laptops with GPUs.\n",
        "\n",
        "A RAG pipeline consists of two main stages: **retrieval** and **generation**. The entire process can be divided into **3 key steps**:\n",
        "\n",
        "1. **Generation of document embeddings**  \n",
        "   In this step, document embeddings are generated from a *corpus* using an embedding model. Each document is encoded into a dense vector representation, capturing semantic information.\n",
        "\n",
        "2. **Document retrieval**  \n",
        "   Relevant documents are fetched from the corpus based on the input query. This is achieved by utilizing the generated embeddings and calculating similarity with the query's embedding to retrieve the most relevant documents.\n",
        "\n",
        "3. **Output generation**  \n",
        "   The retrieved documents are then passed to a **generative model**, which produces contextually relevant responses based on the information extracted from the documents.\n",
        "\n",
        "By leveraging lightweight models and optimizing for efficiency, this RAG pipeline delivers powerful **AI-driven results** even on hardware with **limited resources**.\n",
        "\n",
        "Finally, a **Gradio interface** is provided, allowing users to interact with the system and \"chat\" with the documents they provide, offering a seamless experience for exploring the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzLXqhNeTLO9"
      },
      "source": [
        "## Hardware requirements and constraints\n",
        "\n",
        "Let's find out what hardware we've got available to see what kind of model(s) we'll be able to load. You can also check this with the `!nvidia-smi` command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmd7W4v-WNSD"
      },
      "source": [
        "> **NOTE** : as said above, `sentence-transformers` requires a different version of the `transformers` library, so we update it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yd9LgCsWNSE"
      },
      "outputs": [],
      "source": [
        "%pip install -qqq -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fkHZDXjWNSE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq99vywlTLO-"
      },
      "outputs": [],
      "source": [
        "# Get GPU available memory\n",
        "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN4Pji5BTLO-"
      },
      "source": [
        "Of course, depending on the provided harware, better models can be utilized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMiWvBYzTLO-"
      },
      "source": [
        "### Checking local GPU memory availability\n",
        "\n",
        "Let's first analyze how we decided the model to use for this project. This notebook was primarily run and tested locally on a **laptop** with 16GB of RAM and an NVIDIA RTX 3070 laptop GPU (8GB of VRAM). The main goal was to create a pipeline that could efficiently run on this portable device, leveraging the benefits of GPUs and CUDA for AI tasks while ensuring that the performance and capabilities did not feel lacking compared to larger models.\n",
        "\n",
        "We will need two main ingredients:\n",
        "- An *embedder* model, that calculates dense embeddings from documents\n",
        "- An *LLM*, that provides output given user's queries and the retrieved documents\n",
        "\n",
        "In their dedicated sections we will uncover the choiches made for both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPrgoawNTLO_"
      },
      "source": [
        "### Load our PDF and start producing chunks\n",
        "\n",
        "Let's now start by loading a pdf file and extracting chunks from it. These chunks' **quality** is important, since these will be essentially the *documents* on which we will compute the embeddings on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBaOxo4gaimg"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "PDF_NAME = \"Information Retrieval Slides.pdf\"\n",
        "running_on_colab = False\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata # type: ignore\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Running in Google Colab. Using userdata to get HF_TOKEN.\")\n",
        "    running_on_colab = True\n",
        "except ModuleNotFoundError:\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "    print(\"Not running in Google Colab. Using load_dotenv to get HF_TOKEN.\")\n",
        "\n",
        "file_path = PDF_NAME\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5jUKmVnUnI-"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "# for each document in the 'chunks' folder, we append its text to 'corpus'\n",
        "for file in sorted(os.listdir(\"chunks\")):\n",
        "    with open(os.path.join(\"chunks\", file), \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus.append(f.read())\n",
        "\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBAwgyvTLPA"
      },
      "source": [
        "Let's give a look to some random documents extracted and preprocessed from the initial pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKx35jopaimh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# print some random pages with their indices\n",
        "random.seed(424242)\n",
        "for i in range(5):\n",
        "    doc = random.choice(corpus)\n",
        "    doc_index = corpus.index(doc)\n",
        "    print(f\"DocId: {doc_index}, {doc[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdh2ECCcaimi"
      },
      "outputs": [],
      "source": [
        "print(f\"Average document length: {sum(len(doc) for doc in corpus) / len(corpus)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhCC7pboWNSF"
      },
      "source": [
        "Later, we will use a `json` files containing questions to test the pipeline: it is made up by 69 open-questions we prepared on the PDF topic, in order to asses the final RAG capabilities. For now, we’ll download this file and use it also in this paragraph to ensure the pipeline works as expected.\n",
        "\n",
        "We’ll begin by defining the function `extract_questions(file_path)`, which takes as parameter:\n",
        "\n",
        "- `file_path`: the path to the JSON file containing the questions.\n",
        "\n",
        "At this stage, since our primary goal is to verify that everything functions as intended, we'll just use **some** of the proposed questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR2e6XDsaimi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def extract_questions(file_path):\n",
        "    \"\"\"\n",
        "    Extracts questions from a JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        return [\n",
        "            {\n",
        "                \"question_id\": item.get(\"question_id\", \"\"),\n",
        "                \"question\": item.get(\"question\", \"\")\n",
        "            }\n",
        "            for item in data\n",
        "        ]\n",
        "\n",
        "\n",
        "if not os.path.exists(\"evaluation\"):\n",
        "    os.makedirs(\"evaluation\")\n",
        "    # save locally from https://drive.google.com/file/d/1m2_iG7cGOgRwfVaXzTORU7Pcn71UFKcG/view?usp=drive_link\n",
        "    gdown.download(id=\"1m2_iG7cGOgRwfVaXzTORU7Pcn71UFKcG\", output=\"evaluation/quiz.json\", quiet=False)\n",
        "    # save locally from https://drive.google.com/file/d/1L6rvrbPtwGduaN-8DmpLAWXUtHSfhhF2/view?usp=sharing\n",
        "    gdown.download(id=\"1L6rvrbPtwGduaN-8DmpLAWXUtHSfhhF2\", output=\"evaluation/open_questions.json\", quiet=False)\n",
        "\n",
        "queries = extract_questions(\"evaluation/open_questions.json\")\n",
        "print(f\"Loaded {len(queries)} questions.\")\n",
        "\n",
        "random.seed(4242)\n",
        "print(random.choice(queries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPIvk-TUTLPB"
      },
      "source": [
        "## Gather Dataset Embeddings\n",
        "\n",
        "In this first stage, the aim is to produce the documents embeddings, in order to be able to make similarity searches in the upcoming steps of the pipeline.\n",
        "\n",
        "### About the `cde-small-v1` Model\n",
        "\n",
        "The `cde-small-v1` model, developed by John X. Morris and Alexander M. Rush, is a cutting-edge model for generating **Contextual Document Embeddings (CDE)** ([link](https://huggingface.co/jxm/cde-small-v1)). What sets this model apart is its ability to integrate \"*context tokens*\" into the embedding process, which allows it to capture the nuances and relationships between documents more effectively. This makes it particularly suitable for generating highly accurate embeddings for both documents and queries, especially in cases where capturing the context of a document within the broader corpus is crucial.\n",
        "\n",
        "We chose this model because, as of December 2024, it is one of the leading models under 400M parameters, delivering impressive results on the [**Massive Text Embedding Benchmark (MTEB) leaderboard**](https://huggingface.co/spaces/mteb/leaderboard). Although it ranks 32nd overall, it stands out as the top model in terms of **memory efficiency**, which is a key factor for our project, given the requirement for **limited GPU capabilities**. Additionally, it offers a substantial **embedding dimension of 768**, striking a balance between computational efficiency and embedding quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sg3xXhiaimk"
      },
      "source": [
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "</style>\n",
        "![assets/cde-small-v1.png](https://github.com/giuliocapecchi/LM-project/blob/main/assets/cde-small-v1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqbzIDgTLPC"
      },
      "source": [
        "A key feature of the `cde-small-v1` model is its optimization for a two-stage embedding process:\n",
        "\n",
        "1. **First Stage**: Embedding a subset of documents from the corpus to create \"dataset embeddings,\" which serve as a **reference** for the entire corpus.\n",
        "2. **Second Stage**: Using the dataset embeddings to embed new queries and documents during inference.\n",
        "\n",
        "This model is compact yet delivers solid performance, making it suitable for our use case.\n",
        "\n",
        "### Steps to Gather Dataset Embeddings:\n",
        "\n",
        "1. **Selecting a Subset of Documents**  \n",
        "   We begin by sampling a representative set of documents from the corpus. Following the model's guidelines, we select 512 documents. If this number isn't available, the model can handle oversampling, which is the case for the PDF dataset we're using. Despite this, performance remains strong.\n",
        "\n",
        "2. **Generating Dataset Embeddings**  \n",
        "   After selecting the documents, we encode them using the `cde-small-v1` embedding model. This step produces dense vector representations of the documents, which are representative of the broader corpus.\n",
        "\n",
        "3. **Embedding Queries and Documents**  \n",
        "   Once the dataset embeddings are created, we use them to embed both documents and queries. A key feature of this model is its ability to differentiate between 'queries' and 'documents' during encoding, ensuring context is preserved during the embedding process.\n",
        "\n",
        "\n",
        "Next, we will load the model using the `SentenceTransformers` interface to begin the embedding process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eW5vIiUTLPC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# with sentence-transformers we don't need prefixes but, to do retrieval, we need to use prompt_name=\"query\" and prompt_name=\"document\" in the encode method of the model when embedding queries and documents, respectively.\n",
        "embeddings_model = SentenceTransformer(\n",
        "    \"jxm/cde-small-v1\",\n",
        "    trust_remote_code=True,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W7Dz05MTLPC"
      },
      "source": [
        "During the development of the project, the creators of the embedding model made a change in the Hugging Face repository, which led to unstable results when using the `SentenceTransformer` implementation. If a similar issue arises in the future, it can be mitigated by specifying the `revision` and `tokenizer_kwargs` parameters in the model constructor. These parameters allow to lock the model and tokenizer to a specific branch name, tag, or commit ID from the Hugging Face repository, ensuring stability.\n",
        "\n",
        "Here’s how to implement it:\n",
        "\n",
        "```python\n",
        "embeddings_model = SentenceTransformer(\n",
        "    \"jxm/cde-small-v1\",\n",
        "    trust_remote_code=True,\n",
        "    revision=\"9e2ed1d8d569d34458913d2d246935c1b2324d11\",  # Latest stable model revision\n",
        "    tokenizer_kwargs={\"revision\": \"86b5e0934494bd15c9632b12f734a8a67f723594\"}  # Latest stable tokenizer revision\n",
        ").to(device)\n",
        "```\n",
        "\n",
        "The tags provided above correspond to the latest stable commits (as of December 2024). You can retrieve them directly from the model's card page on Hugging Face, under the \"Files and Versions\" tab.\n",
        "\n",
        "Fortunately for us, the issue was resolved the same day we reported it to the creators via Twitter. We thank them for their prompt response and the fix they provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvDHtnRDTLPC"
      },
      "source": [
        "Let's follow up by producing the `minicorpus` (which is the subsample of the whole corpus) and what we called `dataset_embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ISVKFnGaimk"
      },
      "outputs": [],
      "source": [
        "minicorpus_size = embeddings_model[0].config.transductive_corpus_size # 512\n",
        "random.seed(424242)\n",
        "minicorpus_docs = random.choices(corpus, k=minicorpus_size) # oversampling is okay\n",
        "assert len(minicorpus_docs) == minicorpus_size # We must use exactly this many documents in the minicorpus\n",
        "\n",
        "dataset_embeddings = embeddings_model.encode(\n",
        "    [doc for doc in minicorpus_docs],\n",
        "    prompt_name=\"document\",\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(f\"Corpus size: {len(corpus)}\")\n",
        "print(f\"Computed embeddings for {len(minicorpus_docs)} documents. Shape: {dataset_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWDiKrp7aiml"
      },
      "outputs": [],
      "source": [
        "print(\"Some mini-corpus documents:\")\n",
        "\n",
        "# get some random documents from the minicorpus\n",
        "random.seed(424242)\n",
        "for i in random.sample(range(minicorpus_size), 5):\n",
        "    print(f\"document {i}: {minicorpus_docs[i][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLI5qjGaiml"
      },
      "source": [
        "Now that we have obtained the **dataset embeddings**, we can proceed to embed both documents and queries using the same model.\n",
        "\n",
        "To embed the documents and queries, we must ensure that we specify the correct `prompt_name` for each, as well as pass the `dataset_embeddings` to maintain context. The reason is that, as many state-of-the-art-models, this one was trained with task-specific prefixes:\n",
        "\n",
        "- For documents, use:  \n",
        "  `prompt_name=\"document\"`\n",
        "\n",
        "- For queries, use:  \n",
        "  `prompt_name=\"query\"`\n",
        "\n",
        "We also have to always additionally, specify  the `dataset_embeddings`, in order to use the once we produced before.\n",
        "\n",
        "By doing so, we ensure that the embeddings are generated with the correct context for both retrieval and generation tasks, leveraging the efficient performance of the `cde-small-v1` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-4WdCtLaiml"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = embeddings_model.encode(\n",
        "    [doc for doc in corpus],\n",
        "    prompt_name=\"document\",\n",
        "    dataset_embeddings=dataset_embeddings, # this is the contexualized embeddings of the minicorpus\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "query_embeddings = embeddings_model.encode(\n",
        "    [query['question'] for query in queries],\n",
        "    prompt_name=\"query\",\n",
        "    dataset_embeddings=dataset_embeddings,  # this is the contexualized embeddings of the minicorpus\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQx9Lr3TLPD"
      },
      "source": [
        "We can now computes similarities between the embeddings and all the queries by simply calling `embeddings_model.similarity` (which uses by default cosine similarity), and inspect some of the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8coT3HV9aiml"
      },
      "outputs": [],
      "source": [
        "similarities = embeddings_model.similarity(query_embeddings, doc_embeddings)\n",
        "print(\"'similarities' shape : \",similarities.shape)\n",
        "topk_values, topk_indices = similarities.topk(5)\n",
        "\n",
        "random.seed(424242)\n",
        "random_queries = random.sample(queries, 2)\n",
        "for query in random_queries:\n",
        "    query_idx = queries.index(query)\n",
        "    print(f\"Query: {query['question']}\")\n",
        "    for j, idx in enumerate(topk_indices[query_idx]):\n",
        "        doc = corpus[idx]\n",
        "        print(f\"Rank {j+1} (Score: {topk_values[query_idx][j]:.4f}, Doc ID: {idx}): {doc[:200]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3wLZDIaiml"
      },
      "source": [
        "## Chroma\n",
        "\n",
        "While not strictly necessary for this project - since the provided PDF is relatively small - we decided to integrate a vector database to enhance the storage and retrieval of the embeddings produced. This integration becomes particularly valuable when larger files are processed, as they often require splitting the corpus into smaller chunks, with each chunk generating its own embedding. The resulting large number of embeddings makes efficient storage and retrieval mechanisms increasingly important.\n",
        "\n",
        "After evaluating various options, we selected [**Chroma**](https://www.trychroma.com/) for its simplicity of local deployment and good performances. Chroma is particularly well-suited for managing **large document collections**, providing **faster retrieval** and ensuring that the pipeline remains robust and adaptable to big files.\n",
        "\n",
        "### Implementation Steps\n",
        "\n",
        "We begin by creating a Chroma client and verifying if a collection named `'chroma-collection'` already exists locally:  if it does, we delete it to ensure a clean state before initializing a new one.\n",
        "\n",
        "For reference, a Chroma client can be instantiated in two ways :\n",
        "\n",
        "- in an **ephemeral way** (in-memory), particularly useful for experimentation, such as testing different embedding functions and retrieval techniques. If data persistence is not a requirement, this approach provides a lightweight and efficient way to get started with Chroma while maintaining flexibility for quick iterations. This can be achieved with the following code:\n",
        "\n",
        "    ```python\n",
        "    import chromadb\n",
        "    client = chromadb.Client()\n",
        "    ```\n",
        "\n",
        "- with a **persistent client**. This can be done by providing a  path where Chroma will store its database files on disk, and load them on start.\n",
        "\n",
        "    ```python\n",
        "    import chromadb\n",
        "    client = chromadb.PersistentClient(path=\"/path/to/save/to\")\n",
        "    ```\n",
        "    If a `path` is not provided, the default is `.chroma`\n",
        "\n",
        "More about Chroma can be found in its [documentation](https://docs.trychroma.com/docs/overview/introduction). We choose to utilize the **persistent** approach, so let's follow by instantiating a `PersistentClient`;  if the collection already exists, we delete it to ensure a clean state before initializing a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jszrG73aimm"
      },
      "outputs": [],
      "source": [
        "from chromadb import PersistentClient, Collection\n",
        "\n",
        "COLLECTION_NAME = \"chroma-collection\"\n",
        "\n",
        "# Initialize the Chroma client\n",
        "client = PersistentClient(path=\"./\"+COLLECTION_NAME)\n",
        "\n",
        "# Check if the collection exists and delete it if it does\n",
        "if COLLECTION_NAME in [col.name for col in client.list_collections()]:\n",
        "    client.delete_collection(name=COLLECTION_NAME)\n",
        "    print(f\"Collection {COLLECTION_NAME} exists, deleting it\")\n",
        "else:\n",
        "    print(f\"Collection {COLLECTION_NAME} does not exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2KFyNnBTLPE"
      },
      "source": [
        "### Integration of `cde-small-v1` with Chroma\n",
        "\n",
        "Integrating the **`cde-small-v1`** embedding model with Chroma required the creation of a custom `EmbeddingFunction`. This was necessary because **`cde-small-v1`** has a distinct workflow that deviates from standard embedding models. Specifically, as said it uses:\n",
        "\n",
        "- **`prompt_name`**: This parameter distinguishes between \"document\" and \"query\" embeddings, to have proper alignment during retrieval tasks.\n",
        "- **`dataset_embeddings`**: These provide a reference for conditional embedding generation, improving contextual accuracy in embeddings.\n",
        "\n",
        "By default, Chroma does not natively support these additional parameters. However, it offers the flexibility of extending the `EmbeddingFunction` protocol, allowing us to integrate the specific requirements of the **`cde-small-v1`** model into Chroma’s workflow.\n",
        "\n",
        "We can check the type of the elements of the corpus produced by our chunking functions and use it to differentiate between queries and documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sECLKtWTLPE"
      },
      "outputs": [],
      "source": [
        "print(type(corpus[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGg8Ljrqaimm"
      },
      "outputs": [],
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "\n",
        "class CustomEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        \"\"\"\n",
        "        Perform 'cde-small-v1' embeddings on the input queries.\n",
        "        :param input: Documents or queries to embed.\n",
        "        :return: Embeddings.\n",
        "        \"\"\"\n",
        "        mode = \"query\"\n",
        "\n",
        "        # Generate embeddings using the model with the determined mode\n",
        "        embeddings = embeddings_model.encode(\n",
        "            input,\n",
        "            prompt_name=mode,\n",
        "            dataset_embeddings=dataset_embeddings,\n",
        "            convert_to_tensor=True,\n",
        "        ).cpu().numpy()\n",
        "\n",
        "        return embeddings.tolist()\n",
        "\n",
        "# Create an instance of the embeddings function for queries (we already calculated all the embeddings of the documents)\n",
        "custom_embedding_function = CustomEmbeddingFunction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzGZd21TLPE"
      },
      "source": [
        "Furthermore, while Chroma defaults to using Euclidean distance (`l2`) for nearest-neighbor search, the **`cde-small-v1`** embeddings from `SentenceTransformers` are based on cosine similarity by default. To utilize it, We configure the metadata parameter `\"hnsw:space\": \"cosine\"` when creating the collection. This ensures that Chroma uses cosine similarity, aligning with the previous behavior.\n",
        "\n",
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "    .center {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div class=\"center\">\n",
        "    <img src=\"https://github.com/giuliocapecchi/LM-project/blob/main/assets/chroma-similarity-measures.png?raw=1\" alt=\"chroma-similarity-measures\">\n",
        "</div>\n",
        "\n",
        "Once the collection is configured, we add all the documents and their precomputed embeddings to it.\n",
        "\n",
        "> **NOTE:** We manually computed the embeddings beforehand (for analysis purposes) in the previous steps. If we had only provided the documents, Chroma would have automatically generated the embeddings using the `CustomEmbeddingFunction`, without the need for precomputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIhVPEaFTLPE"
      },
      "outputs": [],
      "source": [
        "# Create a Chroma collection with the custom embedding function\n",
        "collection: Collection = client.create_collection(\n",
        "    name=COLLECTION_NAME,\n",
        "    embedding_function=custom_embedding_function,\n",
        "    get_or_create=True,\n",
        "    metadata={\"hnsw:space\": \"cosine\"} # l2 is the default but cosine is more suitable for this embedding function\n",
        ")\n",
        "\n",
        "# Add the documents and their (already computed) embeddings to the collection\n",
        "collection.add(\n",
        "    ids=[str(i) for i in range(len(corpus))],  # Unique identifiers for the documents\n",
        "    embeddings=doc_embeddings.cpu().numpy(),\n",
        "    #documents=[doc.page_content for doc in corpus],\n",
        "    documents=[doc for doc in corpus],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGxEGrC9aimm"
      },
      "outputs": [],
      "source": [
        "#print(collection.peek()) # returns a list of the first 10 items in the collection\n",
        "print(\"Number of documents in the corpus: \",len(corpus))\n",
        "print(\"Number of documents in the collection: \",collection.count()) # returns the number of items in the collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsm8bDfzTLPE"
      },
      "source": [
        "We can finally test `collection.query` to make some calls to the Chroma collection, and we expect similar (ideally, the same) results as above, when we used `model.similarity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwXd0aw1aimn"
      },
      "outputs": [],
      "source": [
        "query_text = \"How many bytes can UTF-8 use to encode a character?\"\n",
        "\n",
        "# create a Document object with the query text\n",
        "results = collection.query(\n",
        "    query_texts=query_text,\n",
        "    n_results=5\n",
        ")\n",
        "print(results)\n",
        "\n",
        "# Print the results with the document IDs and scores\n",
        "for i, (doc_id, score, doc) in enumerate(zip(results['ids'][0], results['distances'][0], results['documents'][0])):\n",
        "    print(f\"Rank {i+1}: Document ID: {doc_id}, Score: {1-score:.4f}, {doc[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un7yDGUJTLPI"
      },
      "source": [
        "... which is exactly identical to the previous ones.\n",
        "\n",
        "One final thing to note is that in Chroma cosine similarity is calculated as `1 - cosim` ; this is done to ensure that lower values indicate a better position in the ranked list. To obtain results consistent with the previous ones, we subtractde the score from one in the above code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTwJIF5aimn"
      },
      "source": [
        "### Compute scores between queries and documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp-7zfI3TLPJ"
      },
      "source": [
        "Now we can analyze a similarity heatmap between the embeddings of our documents and the provided queries. The results show consistency with the following observations:\n",
        "\n",
        "- **Few documents match the queries**: This suggests that the matching documents are likely the most relevant ones.\n",
        "- **Matches often occur in subsequent documents**: This is expected since the corpus was split into chunks based on the document's structure. For example, chapters are separated, so answers to specific questions are more likely to be located within the same chapter or adjacent sections of the document.\n",
        "\n",
        "Additionally, we provide a distribution of the similarity scores. This helps us assess whether an empirical threshold can be established to determine which documents are truly similar to the provided queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4zQUCwaaimn"
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot heatmap\n",
        "heatmap(similarities.cpu().numpy().T, cmap=\"jet\", ax=axes[0])\n",
        "axes[0].set_title(\"Similarity Heatmap\")\n",
        "\n",
        "# Plot histogram\n",
        "axes[1].hist(similarities.cpu().flatten(), bins=50, color='blue', alpha=0.6)\n",
        "axes[1].set_title(\"Distribution of Similarity Scores\")\n",
        "axes[1].set_xlabel(\"Similarity\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].axvline(x=0.5, color='red', alpha=0.6, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bxLzRQCaimn"
      },
      "source": [
        "## Loading the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld0Lry9LTLPJ"
      },
      "source": [
        "After conducting some research, we chose to work with one of Meta's latest smaller open-source Llama models available at the time of writing this notebook: [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), released the *25th of September 2024*. We specifically selected the *instruct* variant because it is pre-trained to follow basic instructions, offering a more user-friendly and fine-tuned experience compared to the standard non-instruct version (which is also available). Below, we review this model's [specifications](https://llamaimodel.com/requirements-3-2/):\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Llama 3.2 3B Instruct Model Specifications**                    | **Requirement**       | **Details**                                                                                  |\n",
        "|----------------------------------|-----------------------|----------------------------------------------------------------------------------------------|\n",
        "| Parameters                       | 3 billion             |                                                                                              |\n",
        "| Context Length                   | 128,000 tokens        |                                                                                              |\n",
        "| **Hardware Requirements**        |                       |                                                                                              |\n",
        "| CPU and RAM                      |                       | CPU: Multicore processor <br> RAM: Minimum of 16 GB recommended                              |\n",
        "| GPU                              |                       | NVIDIA RTX series (for optimal performance), at least 8 GB VRAM                              |\n",
        "| **Estimated GPU Memory Requirements** |                       |                                                                                              |\n",
        "| Higher Precision Modes           | BF16/FP16             | ~6.5 GB                                                                                      |\n",
        "| Lower Precision Modes            | FP8                   | ~3.2 GB                                                                                      |\n",
        "|                                  | INT4                  | ~1.75 GB                                                                                     |\n",
        "| **Software Requirements**        |                       |                                                                                              |\n",
        "| Software Dependencies            |                       | Frameworks: PyTorch <br> Libraries: Hugging Face Transformers (version 4.45.0 or higher), CUDA |\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVuakpG-TLPJ"
      },
      "outputs": [],
      "source": [
        "print(\"Is bf16 supported: \",torch.cuda.is_bf16_supported())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hw22_isTLPK"
      },
      "source": [
        "The `bitsandbytes` library is a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n",
        "\n",
        "The library includes quantization primitives for 8-bit & 4-bit operations, through `bitsandbytes.nn.Linear8bitLt` and `bitsandbytes.nn.Linear4bit` and 8-bit optimizers through bitsandbytes.optim module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYAVifb8TLPK"
      },
      "source": [
        "#### Quantization Choices\n",
        "\n",
        "We chose a *middle ground* by loading the model with **4-bit quantization** while maintaining **BFloat16** precision for computation.\n",
        "\n",
        "The use of 4-bit quantization reduces the precision of the model’s weights to just 4 bits per value, significantly lowering memory usage and accelerating inference. This method retains only the most essential information, sacrificing some numerical precision, but allows for larger models to be handled on GPUs with limited memory. Despite the weights being quantized to 4 bits, the model still performs computations in **16-bit floating point (BFloat16)** precision. BFloat16 is a 16-bit format that preserves much of the dynamic range of floating-point operations while requiring less memory than the traditional 32-bit format. This way, while the 4-bit quantization reduces the memory footprint of the model weights, the computation is performed in BFloat16, optimizing performance on modern GPUs that are tailored for BFloat16 operations. This configuration strikes a balance between computational efficiency and numerical precision, enabling fast inferences with minimal memory usage without significant loss in result quality.\n",
        "\n",
        "Given the hardware constraints, we opted for **4-bit quantization** using `BitsAndBytes` (as explained later). This approach greatly reduces the memory footprint and speeds up inference, ensuring acceptable performance for our use case. Without this configuration, the GPU’s memory usage was consistently at 100%, and inference times were approximately 2-3 minutes per query. With 4-bit quantization, memory usage drops to about 6GB, preventing GPU overload and reducing inference time to around 30 seconds per query, delivering satisfactory results. Further details on the quantization process will be provided later.\n",
        "\n",
        "However, we also need to consider that both the embedding model and the LLM must be loaded into memory, which adds another layer of complexity to the memory management. This requires careful balancing, as the total memory usage must accommodate both the LLM and the embedding model simultaneously. We will address how we manage this in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnpvz_Wxaimn"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Quantization is a technique that reduces the precision of the model’s weights to make it run faster and consume less memory, often at the cost of a slight reduction in model accuracy or quality\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                      # Lower precision reduces memory usage and can speed up inference (maybe try 8)\n",
        "    bnb_4bit_use_double_quant=True,         # Using double quantization can help reduce the loss in accuracy associated with quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",              # Normal Float 4-bit quantization, a scheme that may preserve model quality better than straightforward quantization methods\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # The internal compute dtype used during inference. bfloat16 (BF16) is often chosen because it’s efficient on modern accelerators\n",
        "    llm_int8_enable_fp32_cpu_offload=True   # Enable FP32 CPU offload\n",
        ")\n",
        "\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map={\"\": device},  # ensure all modules are on GPU\n",
        "    quantization_config=bnb_config,\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = llm_model.generation_config\n",
        "generation_config.max_new_tokens = 500                    # the maximum number of new tokens the model will generate (long outputs might be more prone to off-topic or repetitive content)\n",
        "generation_config.min_new_tokens = 10                     # the minimum number of new tokens the model will generate\n",
        "generation_config.temperature = 0.7                       # it controls the randomness of the generation, lower temp means more deterministic, conservative (less creative) and repetitive answers [about 0.1-1.2]\n",
        "generation_config.top_p = 0.7                             # nucleus sampling controls how the model picks words based on their cumulative probability, lower value (0.5) means safer, more coherent text but less diverse [about 0.5-0.9]\n",
        "generation_config.num_return_sequences = 1                # how many separate output sequences are returned for each generation prompt, get multiple different answers in one go, useful for picking the best response from several tries\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id   # special token IDs that represent padding and the end-of-sequence token. Generally, these are set to ensure the model knows when to stop and how to handle inputs of different lengths\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.repetition_penalty = 1.2                # discourages the model from repeating the same phrases or tokens over and over [about 1.0-2.0] (high value cause the model to avoid some tokens even if they are contextually appropriate)\n",
        "generation_config.num_beams = 5                           # the number of beams used in beam search, higher value means more diverse answers but also slower generation\n",
        "generation_config.early_stopping = True                   # whether to stop the beam search when at least num_beams sentences are finished per batch or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1vCxFFxWNSM"
      },
      "source": [
        "We loaded various things on the GPU up to know. Still, with the proposed configuration for all the pipeline components, GPU usage should be acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNwOi2dHTLPK"
      },
      "outputs": [],
      "source": [
        "# Check GPU usage\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # in GB\n",
        "memory_allocated_gb = torch.cuda.memory_allocated(device) / (1024 ** 3)  # in GB\n",
        "print(f\"Memory Allocated: {memory_allocated_gb:.2f} GB\")\n",
        "print(f\"Total memory usage: {(memory_allocated_gb / gpu_memory_gb) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTIj3ob0aimn"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_documents(query, k=5):\n",
        "    results = collection.query( # query the Chroma collection\n",
        "        query_texts=query,\n",
        "        n_results=k\n",
        "    )\n",
        "    return results['documents'][0], results['distances'][0]\n",
        "\n",
        "# test the function\n",
        "query = \"How many bytes can UTF-8 use to encode a character?\"\n",
        "print(f\"Query: {query}\")\n",
        "documents, distances = retrieve_relevant_documents(query)\n",
        "for doc, distance in (zip(documents, distances)):\n",
        "    print(f\"(score: {1-distance:.4f}) {doc[:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZoEEO3caimo"
      },
      "outputs": [],
      "source": [
        "base_prompt = '''You are an AI assistant expert of Information Retrieval.\n",
        "Your task is to provide answers to user questions based on the provided context.\n",
        "\n",
        "Instructions:\n",
        "- Use the provided context to construct your answers.\n",
        "- Avoid directly quoting examples or specific details from the context unless they are explicitly required to answer the question.\n",
        "- Paraphrase any necessary details from the context in a way that does not depend on the user's knowledge of the full context.\n",
        "- If the context lacks sufficient information, provide the best possible answer using general knowledge or state: \"The provided context does not have the answer.\"\n",
        "\n",
        "Your goal is to ensure that your response is complete and clear even if the user has no access to the context.\n",
        "\n",
        "User question: {user_query}\n",
        "\n",
        "Provided Context:\n",
        "{chunks_information}\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdnGxkarWNSM"
      },
      "source": [
        "## Perform Queries to the RAG Pipeline\n",
        "\n",
        "Now, we're ready to test the pipeline. The following function executes a query by performing the steps outlined below:\n",
        "\n",
        "1. **Retrieve Relevant Documents**: The function invokes `retrieve_relevant_documents` (defined in the coresponding section) to fetch the top 5 relevant documents to the query, along with their similarity scores\n",
        "2. **Filter Documents**: Documents with a similarity score below 0.5 (calculated as `1 - score`) are discarded, retaining only those with sufficient relevance. If no documents meet the relevance threshold, a message indicating that no relevant documents were found is returned.\n",
        "3. **Print the results**: If `print_retrieved_documents` is set to `True`, retrieved relevant documetns are also printed on the terminal.\n",
        "4. **Prepare the Prompt**: The filtered documents are used to create a prompt, which is passed to the model to generate a response. The prompt includes the user's query and the relevant document chunks.\n",
        "5. **Model Inference**: The prompt is tokenized and passed to the RAG model. The model generates a response using beam search (with 5 beams) and applies early stopping.\n",
        "6. **Generate and Return Response**: The function decodes the generated tokens and returns the resulting sequence as the response.\n",
        "\n",
        "This process ensures that the generated response is grounded in relevant documents, providing more accurate, context-aware answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ndL7blvWNSN"
      },
      "outputs": [],
      "source": [
        "def query_rag_model(query, base_prompt=base_prompt, print_retrieved_documents=True):\n",
        "    \"\"\"\n",
        "    Passes the user query to the RAG model and returns the generated answer.\n",
        "    \"\"\"\n",
        "\n",
        "    documents, scores = retrieve_relevant_documents(query, k=5)\n",
        "\n",
        "    if print_retrieved_documents:\n",
        "        print(\"Filtered documents and their scores: \")\n",
        "        for doc, score in zip(documents, scores):\n",
        "            print(f\"Score: {1-score:.4f}, {doc}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # Filters out documents with a similarity score below 0.5\n",
        "    filtered_documents = [doc for doc, score in zip(documents, scores) if 1 - score >= 0.5]\n",
        "\n",
        "    # If no relevant documents are found, return a message\n",
        "    if not filtered_documents:\n",
        "        print(f\"No relevant documents found for query: {query}\")\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    # Prepare the prompt for the model\n",
        "    prompt = base_prompt.format(user_query=query, chunks_information=\"\\n\".join(filtered_documents))\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(\"PROMPT:\", prompt)\n",
        "\n",
        "    with torch.inference_mode(): # disables gradient computation during model execution\n",
        "        outputs = llm_model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "    generated_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    return generated_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWcv6y7WNSN"
      },
      "source": [
        "Try it out by changing the query! Try also to change `print_retrieved_documents` to `True` to see what documents are influencing the produced output.\n",
        ">This operation takes ~15/30 seconds on our machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPqDR2R0WNSN"
      },
      "outputs": [],
      "source": [
        "response = query_rag_model(\"what is information retrieval?\", base_prompt=base_prompt, print_retrieved_documents=False)\n",
        "print(\"Response: \", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU5nIjpEWNSN"
      },
      "source": [
        "# Evaluating model responses\n",
        "\n",
        "The following section covers experiments based on how the model replies to **open-answer** questions.\n",
        "\n",
        "Questions were stored in this format, and were manually tailored by us in order to asses the capabilities of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Tfvfg0WNSN"
      },
      "source": [
        "The file,  `open_questions.json` file contains open-answer questions, where an entry looks like:\n",
        "\n",
        "```json\n",
        "{\n",
        "        \"question_id\": \"1\",\n",
        "        \"question\": \"What is Jaccard Coefficient?\"\n",
        "}, ...\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Field**       | **Content**                                     |\n",
        "|------------------|-------------------------------------------------|\n",
        "| `question_id`    | Unique identifier of the question        |\n",
        "| `question`       | The question text                              |\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W-q3VYEWNSN"
      },
      "outputs": [],
      "source": [
        "evaluation_folder_path = \"./evaluation/\"\n",
        "questions_file_path = evaluation_folder_path + \"open_questions.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2U0YrdmWNSN"
      },
      "outputs": [],
      "source": [
        "open_questions = extract_questions(questions_file_path)\n",
        "\n",
        "# TODO : rimuovi questo, serve solo per test veloce\n",
        "open_questions = open_questions[:1]\n",
        "\n",
        "print(f\"Loaded {len(open_questions)} open questions.\")\n",
        "print(open_questions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMurWVAJWNSO"
      },
      "source": [
        "### Evaluating the model on open-questions\n",
        "\n",
        "`process_open_questions(queries, folder_path)` is a function used to run experiments over a list of open-answer questions, taking as input the list of questions with `queries` and folder path `folder_path` for results storing. It does the following:\n",
        "\n",
        "1. Delete previous results if already done, create `res` folder otherwise\n",
        "2. Process one-by-one queries via `response = query_rag_model(query, base_prompt)`\n",
        "\n",
        "Note that:\n",
        "* No results are returned since are already store on `.txt` files\n",
        "* retrieve_relevant_documents is called only to write on .txt files documents retrieved, but it is also called into `query_rag_model` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcVeNt8AWNSO"
      },
      "outputs": [],
      "source": [
        "def process_open_questions(queries, folder_path, model=\"RAG\"):\n",
        "    \"\"\"\n",
        "    Processes a list of open questions, retrieves relevant documents,\n",
        "    and saves the responses to text files in a specified folder.\n",
        "\n",
        "    Parameters:\n",
        "    - queries (list): List of queries to process.\n",
        "    - folder_path (str): Path to the folder where results will be saved.\n",
        "    - model (str): Name of the model to use for processing the queries.\n",
        "    \"\"\"\n",
        "    results_folder = os.path.join(folder_path, model+\"-open-questions\")\n",
        "    if os.path.exists(results_folder):\n",
        "        # Clear the folder if it exists\n",
        "        for file_name in os.listdir(results_folder):\n",
        "            file_path = os.path.join(results_folder, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "    else:\n",
        "        # Create the folder if it doesn't exist\n",
        "        os.makedirs(results_folder)\n",
        "\n",
        "    # Process each query and save the response\n",
        "    for idx, query in enumerate(tqdm(queries, desc=\"Processing queries\"), start=1):\n",
        "            if model ==\"RAG\":\n",
        "                # Get the model's response\n",
        "                response = query_rag_model(query['question'], base_prompt, print_retrieved_documents=False)\n",
        "            elif model == \"LLM\":\n",
        "                prompt = \"You are an AI assistant expert of Information Retrieval.\\n\"+ query['question']\n",
        "                encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "                with torch.inference_mode(): # disables gradient computation during model execution\n",
        "                    outputs = llm_model.generate(\n",
        "                        input_ids=encoding.input_ids,\n",
        "                        attention_mask=encoding.attention_mask,\n",
        "                        generation_config=generation_config,\n",
        "                    )\n",
        "\n",
        "                generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "                response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "            else:\n",
        "                raise ValueError(\"Invalid model name. Use either 'RAG' or 'LLM'.\")\n",
        "\n",
        "            # Construct the file path for the current query\n",
        "            file_path = os.path.join(results_folder, f\"open_question_{idx}.mmd\")\n",
        "\n",
        "            # Save the query, response, and documents to a text file\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(f\"Query {idx}: {query['question']}\\n\\n\")\n",
        "                file.write(f\"Reply {idx}: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB6_0hr_WNSO"
      },
      "outputs": [],
      "source": [
        "process_open_questions(open_questions, evaluation_folder_path, model=\"RAG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJiQS1_rWNSO"
      },
      "outputs": [],
      "source": [
        "process_open_questions(open_questions, evaluation_folder_path, model=\"LLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Procedure\n",
        "As reported above, to evaluate the performance of the RAG pipeline we conducted a manual assessment using a set of **60 open-ended questions** related to information retrieval. The evaluation consisted of two phases:\n",
        "1. RAG Model Testing:\n",
        "   - The RAG pipeline was used to answer the questions, incorporating both retrieved documents and the language model's generation capabilities.\n",
        "   - The pipeline was designed to act as an \"AI assistant\" and \"information retrieval expert,\" as explicitly stated in the prompt.\n",
        "2. Base Model Testing:\n",
        "   - The same 60 questions were tested using the base language model without the RAG pipeline.\n",
        "   - To ensure a fair comparison, efforts were made to align the prompts of the two models. For the base model, the adjusted prompt explicitly mentioned the \"information retrieval\" context to avoid bias in favor of the RAG pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### Results\n",
        "\n",
        "| **Evaluation Metric**    | **RAG Pipeline** | **Base Model** |\n",
        "|-|-|-|\n",
        "| Total Questions Evaluated | 60| 60|\n",
        "| Correct Answers           | 57| 23|\n",
        "| Accuracy (%)              | 95%|  38.3%|\n",
        "\n",
        "---\n",
        "\n",
        "#### Analysis of Results:\n",
        "1. The RAG pipeline demonstrated a 95% accuracy rate, significantly outperforming the base model, which achieved only 38.3% accuracy.\n",
        "2. This substantial improvement highlights the value of integrating a retrieval mechanism into the pipeline. By grounding the model's responses in relevant documents, the RAG pipeline could produce more accurate and contextually aligned answers.\n",
        "\n",
        "3. For the base model, the incorrect answers were due to:\n",
        "     - Hallucinations: The model generated plausible-sounding but incorrect information.\n",
        "     - Lack of Specificity: Responses were vague or unrelated to the query.\n",
        "4. In contrast, the RAG pipeline failed in only 3 cases where the answers were indeed correct, but they didn't come from the given context but from general knowledge, so we labled them as incorrect.\n"
      ],
      "metadata": {
        "id": "fryWDCryZe00"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAGA5pl0WNSO"
      },
      "source": [
        "## TODO : Manca commento risultati domande aperte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ1KmZNwWNSO"
      },
      "source": [
        "# Gradio Interface <div align=\"center\"><img src=\"https://www.gradio.app/_app/immutable/assets/gradio.CHB5adID.svg\" alt=\"Gradio Logo\" width=\"200\"></div>\n",
        "\n",
        "Lastly, we provide a **Gradio** interface to make interacting with the RAG pipeline both user-friendly and accessible. [Gradio](https://gradio.app) is a Python library that enables developers to quickly create customizable, interactive web-based interfaces. It is widely appreciated for its simplicity and flexibility.\n",
        "\n",
        "The interface is launched in the next cell through the final command `demo.launch(debug=True)`\n",
        "\n",
        "> This will also start a local server and generate a link (usually `http://127.0.0.1:7860`) that you can open in your browser. On Colab this will be different, but the you can find the url in the console output\n",
        "\n",
        "The interface is straightforward:\n",
        "- You can type your questions or prompts for the RAG pipeline into the **input field**.\n",
        "- The interface estimated inference waiting time (based on the last response waiting time), then the generated answer.\n",
        "\n",
        "If the interface is launched with the `debug=True` option (as in this case), it will also print the retrieved documents and their scores in the console; this can be useful for gaining insights into the system's behavior. Otherwise, the console prints are suppressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyeIkuECWNSO"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "latex_delimiters = [\n",
        "    {\"left\": \"\\\\[\", \"right\": \"\\\\]\", \"display\": True},  # Formulas in display mode\n",
        "    {\"left\": \"\\\\(\", \"right\": \"\\\\)\", \"display\": False},  # Formulas in inline mode\n",
        "]\n",
        "\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    .input-box { border: 1px solid #ccc; border-radius: 4px; padding: 10px; margin: 10px 0; }\n",
        "    .output-box { border: 1px solid #ccc; border-radius: 4px; padding: 10px; margin: 10px 0; background-color: #373535; height: auto; }\n",
        "\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"# RAG Model Query Interface\")\n",
        "    gr.Markdown(f\"Ask questions to the RAG model and get answers based on the provided PDF context (*{PDF_NAME}*).\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_text = gr.Textbox(\n",
        "                label=\"Enter your query\",\n",
        "                placeholder=\"Type your question here...\",\n",
        "                elem_classes=[\"input-box\"]\n",
        "            )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_text = gr.Markdown(\n",
        "                label=\"Answer\",\n",
        "                elem_classes=[\"output-box\"],\n",
        "                latex_delimiters=latex_delimiters\n",
        "                )\n",
        "\n",
        "\n",
        "    input_text.submit(\n",
        "        fn=query_rag_model,\n",
        "        inputs=input_text,\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "# launch the interface\n",
        "demo.launch(debug=True, show_error=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im415tVOWNSP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "lm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}