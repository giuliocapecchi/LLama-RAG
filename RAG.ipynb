{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliocapecchi/LM-project/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation (RAG) for *Information Retrieval* Query Answering\n",
        "\n",
        "This project implements a **Retrieval-Augmented Generation (RAG)** system, a modern technique that integrates document retrieval and answer generation. The RAG approach enhances the performance of large language models (LLMs) by incorporating external information to improve the model's ability to answer queries outside its original training data. This is especially beneficial for businesses or single users that cannot afford to build a ChatGPT-like LLM *from scratch*. RAG provides a faster and more efficient alternative than re-training, as it tries to expand the model knowledge by simply providing relevant documents as data sources to the user's queries.\n",
        "\n",
        "**The use cases of RAGs** include:\n",
        "- Chatbots\n",
        "- Document-based question answering systems\n",
        "- Legal and technical document summarization\n",
        "- many more\n",
        "\n",
        "In this project, we will develop a **chatbot-like** system where users can upload PDF documents and ask the LLM about them. Particularly, we tried to balance good performances with the necessity to utilize *light* (computationally speaking) models. The results were in the end quite satisfying.\n",
        "\n",
        "\n",
        "The proposed pipeline will:\n",
        "\n",
        "1. Split the documents into smaller chunks (with special handling for *mathematical formulas*).\n",
        "2. Embed these chunks using a model from Hugging Face (`cde-small-v1`).\n",
        "3. Initialize a (quantized) local copy of `LLaMA 3.2 3B` for answer generation.\n",
        "4. Provide access to the model trough a `Gradio` interface.\n",
        "\n",
        "More details will be present in their respective sections.\n",
        "\n",
        "\n",
        "In the following, we will delve into the technical details and results obtained from implementing this pipeline, including performance analysis and the metrics used to evaluate the system's effectiveness.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OV1QFHWLvIw"
      },
      "source": [
        "## PDF Preprocessing and Documents Creations\n",
        "\n",
        "One of the critical challenges in preprocessing documents for the implementation of a RAG system is ensuring accurate and efficient data extraction while maintaining contextual integrity. To address this, we developed a pipeline that tries to handle various aspects of document preprocessing, leveraging open-source tools to achieve good results.\n",
        "\n",
        "The first phase regards the preprocessing and splitting of the chosen PDF file. To accomplish this, we propose a hybrid approach that combines two tools:  \n",
        "- [**Unstructured**](https://github.com/Unstructured-IO): An open-source tool for converting unstructured data into **structured** outputs.   \n",
        "- [**Nougat**](https://github.com/facebookresearch/nougat  ): A Transformer-based OCR model designed to simplify the conversion of complex content into a machine-readable format.   \n",
        "\n",
        "The pipeline integrates these as follows:  \n",
        "1. **Document Parsing**: Using `Unstructured`'s partition function, we extracted elements characterized by type and content. This allowed us to:  \n",
        "   * Filter irrelevant or meaningless elements.  \n",
        "   * identify **mathematical formulas**, since they will have a special type\n",
        "   * Translate elements into a **dataframe with columns for Type and Value** for easier handling.  \n",
        "\n",
        "2. **Enhanced Content Extraction**: Pages requiring additional attention (e.g., those with complex elements such as formulas) were processed using `Nougat`. This converts them into text formatted in Markdown.\n",
        "\n",
        "By combining the strengths of `Unstructured` and `Nougat`, this approach enables the creation of good quality, structured document representations while addressing challenges like text cleaning, splitting, and chunking.\n",
        "\n",
        "### Computational Considerations and Simpler Alternatives\n",
        "\n",
        "These preprocessing steps, particularly those involving splitting and structuring complex documents, are computationally demanding. This pipeline is most effective for **academic paper-style documents**, which often feature intricate structures or a mix of diverse content types, and are often just a few pages long.\n",
        "\n",
        "For documents primarily composed of simple text, **lighter and faster alternatives** exist. For instance, the [Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/) provided by LangChain offer a simpler, yet effective, solution for text-based document splitting, significantly reducing processing time and resource usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmLEt9LCLpLn"
      },
      "source": [
        "Let's dive into the implementation. Let's start by installing required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j3UrpIY5sLgF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nougat-ocr 0.1.17 requires timm==0.5.4, but you have timm 1.0.12 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qqq torch gdown huggingface_hub python-dotenv pymupdf python-Levenshtein nltk tqdm unidecode gradio bitsandbytes seaborn unstructured[pdf] --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QRv6r2rWNRw"
      },
      "source": [
        "We start off by downloading the 'assets' folder, that contains images rendered in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f-lBuhHdWNRw",
        "outputId": "9103e8d2-64d9-451d-a0fc-c82742a44233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assets folder 'assets' already exists. Skipping download and extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "zip_url = \"https://drive.google.com/uc?export=download&id=1iira8TFvy7Nix3NwusLMLqgTCmjy76Ru\"\n",
        "zip_file_name = \"assets.zip\"\n",
        "output_dir = \"assets\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    print(\"Downloading assets...\")\n",
        "    response = requests.get(zip_url, stream=True)\n",
        "    with open(zip_file_name, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download completed.\")\n",
        "\n",
        "    # Unzip the assets\n",
        "    print(\"Unzipping assets...\")\n",
        "    with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    print(f\"Assets unzipped to '{output_dir}'.\")\n",
        "\n",
        "    os.remove(zip_file_name)\n",
        "    print(\"Cleanup completed.\")\n",
        "else:\n",
        "    print(f\"Assets folder '{output_dir}' already exists. Skipping download and extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FPsNo-aWNRy"
      },
      "source": [
        "### Installing Additional Required Libraries\n",
        "\n",
        "On Colab, the installations above are sufficient. However, on **Windows** and **macOS**, additional steps are required to install libraries necessary for both Unstructured and Nougat:\n",
        "\n",
        "- **Poppler**: a PDF rendering library.  \n",
        "- **Tesseract**: an open-source OCR engine.\n",
        "\n",
        "\n",
        "#### **Steps for Windows**\n",
        "The easiest way to install the required libraries on Windows is via Conda (assuming Conda is installed):\n",
        "\n",
        "1. **Install the required packages**:\n",
        "    ```bash\n",
        "    %conda install -c conda-forge poppler tesseract -y\n",
        "    %pip install -q python-magic-bin==0.4.14\n",
        "    ```\n",
        "\n",
        "2. **Add Tesseract to the system PATH**:\n",
        "    Use the following Python code to set up the `TESSDATA_PREFIX` environment variable:\n",
        "\n",
        "    ```python\n",
        "    import os, sys\n",
        "\n",
        "    # Get the path of the current Conda environment\n",
        "    conda_env_path = os.path.dirname(sys.executable)\n",
        "    tessdata_path = os.path.join(conda_env_path, \"share\", \"tessdata\") # the necessary files should be here\n",
        "\n",
        "    # Set the TESSDATA_PREFIX environment variable\n",
        "    os.environ[\"TESSDATA_PREFIX\"] = os.path.join(conda_env_path, \"share\")\n",
        "    print(\"TESSDATA_PREFIX:\", os.environ[\"TESSDATA_PREFIX\"])\n",
        "\n",
        "    # Verify the tessdata directory\n",
        "    if os.path.exists(tessdata_path):\n",
        "        print(f\"Tessdata directory found: {tessdata_path}\")\n",
        "    else:\n",
        "        print(f\"Tessdata directory not found: {tessdata_path}\")\n",
        "    ```\n",
        "\n",
        "#### **Steps for macOS**\n",
        "On macOS, installation is simpler. Use the following command to install the required libraries (using `brew`):\n",
        "\n",
        "1. **Install the required packages**:\n",
        "    ```bash\n",
        "    brew install poppler tesseract\n",
        "    ```\n",
        "\n",
        "2. **Verify the installation**:\n",
        "    Check if Tesseract is properly installed by running:\n",
        "    ```bash\n",
        "    tesseract --version\n",
        "    ```\n",
        "\n",
        "\n",
        "As said, this approach serves as an enhancement to the RAG pipeline, enabling more accurate extraction from PDFs, particularly for elements like mathematical formulas in scientific papers. However, it is not mandatory; if the PDF contains mostly plain text, a simpler chunking method may be sufficient and more straightforward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo9QzZJzSLKe"
      },
      "source": [
        "### Downloading the Selected PDF for Analysis\n",
        "\n",
        "In this notebook, we will use slides from our *Multimedia and Information Retrieval* course. These slides present a challenging objective due to their inclusion of complex mathematical formulas, making accurate parsing a non-trivial task. In the next cell we will download the chosen PDF. You are free to select any PDF of your preference; however, keep in mind that the evaluation phase will of course vary depending on the content of the document. \n",
        "\n",
        "--- \n",
        "> **Note:** To speed up execution, we’ll also download the `elements.pkl` file to bypass some expensive processing (which would otherwise take approximately TODO : RIGUARDA *20/30 minutes* for ~300 pages)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x9hQwckZHXHO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Information Retrieval Slides.pdf already exists.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "\n",
        "PDF_NAME = \"Information Retrieval Slides.pdf\"\n",
        "if os.path.exists(PDF_NAME):\n",
        "    print(f\"File {PDF_NAME} already exists.\")\n",
        "else:\n",
        "    # save locally from https://drive.google.com/file/d/1xUA6_ZBJzWGF7kWpM1YZTK3R1siYg1qY/view?usp=drive_link\n",
        "    gdown.download(id=\"1xUA6_ZBJzWGF7kWpM1YZTK3R1siYg1qY\", output=PDF_NAME, quiet=False)\n",
        "\n",
        "    # download of pickled elements to speed up the execution\n",
        "    gdown.download(id=\"17eXYgmiTL9-f9F5vIT5j7QffHIu_QwuN\", output=\"elements.pkl\", quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_HRybt8S-Xp"
      },
      "source": [
        "## PDF Partitioning with Unstructured\n",
        "\n",
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "    .center {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "    }\n",
        "    .center img {\n",
        "        width: 50%;\n",
        "   }\n",
        "</style>\n",
        "\n",
        "<div class=\"center\">\n",
        "        <img src=\"assets/unstructured.png\" alt=\"unstructured\">\n",
        "</div>\n",
        "\n",
        "\n",
        "This tool processes the PDF using the `partition` function to extract structured elements.\n",
        "\n",
        "It provides various strategies, but we decide to apply the **`hi_res`** strategy because is highly sensitive and it is recommended if you want obtain precise classifications for document elements. Other ones were `fast` and `ocr_only`, but didn't produce acceptable results. Adopting this strategy, `Unstructured` was able to use the document layout to gain additional information about document elements, in particular it can recognize:\n",
        "\n",
        "- **Titles** and **Text** (we use them as it is)\n",
        "- **Formulas** (we further process them)\n",
        "- **Page Breaks** (fundamental create chuncks)\n",
        "- **Images** (will be removed)\n",
        "\n",
        "\n",
        "As said, Unstructured can be computationally quite extensive, especiallt with the `hi_res` strategy; that is why we downloaded `elements.pkl` above, that contains the results produced by Unstructured on the target PDF. In the following cell, set `load_elements_from_pickle = False` to recompute them (takes approximately TODO: CHECKA ~10/15 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NpVjk7JfIoTh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of recognized elements: 4486 in file Information Retrieval Slides.pdf\n"
          ]
        }
      ],
      "source": [
        "from unstructured.partition.auto import partition\n",
        "from tqdm import tqdm\n",
        "import fitz  # PyMuPDF\n",
        "import pickle\n",
        "\n",
        "\n",
        "load_elements_from_pickle = True\n",
        "elements = []\n",
        "\n",
        "if load_elements_from_pickle:\n",
        "    # load elements from pickle\n",
        "    with open('elements.pkl', 'rb') as f:\n",
        "        elements = pickle.load(f)\n",
        "else: # process the PDF file\n",
        "    doc = fitz.open(PDF_NAME)  # open the PDF file\n",
        "    total_pages = len(doc)\n",
        "\n",
        "    for page_number in tqdm(range(total_pages), desc=\"Processing PDF pages\"):\n",
        "        # extract the single page\n",
        "        temp_doc = fitz.open()\n",
        "        temp_doc.insert_pdf(doc, from_page=page_number, to_page=page_number)\n",
        "\n",
        "        # save the page to a temporary file\n",
        "        temp_page_file = f\"temp_page_{page_number}.pdf\"\n",
        "        temp_doc.save(temp_page_file)\n",
        "        temp_doc.close()\n",
        "\n",
        "        # apply the partitioning to the page\n",
        "        page_elements = partition(\n",
        "            filename=temp_page_file,\n",
        "            strategy=\"hi_res\",\n",
        "            skip_infer_table_types=[],\n",
        "            include_page_breaks=True\n",
        "        )\n",
        "        elements.extend(page_elements)  # add the elements to the list\n",
        "        # finally, delete the temporary file\n",
        "        os.remove(temp_page_file)\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "# print the number of recognized elements\n",
        "print(\"Number of recognized elements: \" + str(len(elements)) + \" in file \" + PDF_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CITylvMsd1Mf"
      },
      "source": [
        "Now store the elements in a pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k_6QJqgxj4ou"
      },
      "outputs": [],
      "source": [
        "with open('elements.pkl', 'wb') as f:\n",
        "    pickle.dump(elements, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfWUBRLfjAK"
      },
      "source": [
        "## Filtering and Analyzing Extracted Elements\n",
        "\n",
        "Here we build a **dataframe**, creating pairs of **Type** and **Value**, using the elements extracted from the PDF file.\n",
        "\n",
        "Then we print all the different element's Type recognized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MYMr613sfcqH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<class 'unstructured.documents.elements.Title'>\n",
            " <class 'unstructured.documents.elements.EmailAddress'>\n",
            " <class 'unstructured.documents.elements.PageBreak'>\n",
            " <class 'unstructured.documents.elements.ListItem'>\n",
            " <class 'unstructured.documents.elements.NarrativeText'>\n",
            " <class 'unstructured.documents.elements.Image'>\n",
            " <class 'unstructured.documents.elements.FigureCaption'>\n",
            " <class 'unstructured.documents.elements.Table'>\n",
            " <class 'unstructured.documents.elements.Text'>\n",
            " <class 'unstructured.documents.elements.Footer'>\n",
            " <class 'unstructured.documents.elements.Formula'>\n",
            " <class 'unstructured.documents.elements.Header'>]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_elements = pd.DataFrame( [[type(elem), elem.text] for elem in elements], columns=['Type', 'Value'])\n",
        "\n",
        "# Print all the unique types of elements\n",
        "print(df_elements['Type'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuoY63qTQy4"
      },
      "source": [
        "Next, we clean the dataframe by filtering out elements that are not useful for our pipeline:  \n",
        "- **Images**: While OCR techniques extract text from images, the resulting content often lacks meaningful context and is not suitable for the language model.  \n",
        "- **Tables**: Similar to images, the extracted text from tables often loses its structural meaning and context.  \n",
        "- **Footers** and **Headers**: These typically contain repetitive or irrelevant information that does not contribute to the main content.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q6iG38ZWwv7L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of actual elements: 4486\n",
            "Number of elements after filtering: 4031\n",
            "Remaining Types left in the dataframe:\n",
            " [<class 'unstructured.documents.elements.Title'>\n",
            " <class 'unstructured.documents.elements.EmailAddress'>\n",
            " <class 'unstructured.documents.elements.PageBreak'>\n",
            " <class 'unstructured.documents.elements.ListItem'>\n",
            " <class 'unstructured.documents.elements.NarrativeText'>\n",
            " <class 'unstructured.documents.elements.FigureCaption'>\n",
            " <class 'unstructured.documents.elements.Text'>\n",
            " <class 'unstructured.documents.elements.Formula'>]\n"
          ]
        }
      ],
      "source": [
        "from unstructured.documents.elements import Image, Table, Footer, Header, PageBreak, Formula\n",
        "\n",
        "# print df number of rows\n",
        "print(\"Number of actual elements: \" + str(len(df_elements)))\n",
        "\n",
        "# drop all the rows with unuseful Types\n",
        "df_elements = df_elements[df_elements['Type'] != Image]\n",
        "df_elements = df_elements[df_elements['Type'] != Table]\n",
        "df_elements = df_elements[df_elements['Type'] != Footer]\n",
        "df_elements = df_elements[df_elements['Type'] != Header]\n",
        "df_elements = df_elements.drop(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')].index)\n",
        "\n",
        "# print df number of rows\n",
        "print(\"Number of elements after filtering: \" + str(len(df_elements)))\n",
        "\n",
        "# reassign the dataframe index\n",
        "df_elements = df_elements.reset_index(drop=True)\n",
        "\n",
        "# print all the different possible Type in the dataframe\n",
        "print(\"Remaining Types left in the dataframe:\\n\",df_elements['Type'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aswk1lwSlziI"
      },
      "source": [
        "Check if there are some empty-valued elements, that are not Page Breaks, in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7vBne_I70vij"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# print rows of df_elements with bith Type != PageBreak and Value empty\n",
        "print(len(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')]))\n",
        "\n",
        "if len(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')]) > 0:\n",
        "  df_elements = df_elements.drop(df_elements[(df_elements['Type'] != PageBreak) & (df_elements['Value'] == '')].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrkLBolTsQv"
      },
      "source": [
        "## Filtering Pages Containing Formulas in a DataFrame\n",
        "\n",
        "This code processes a DataFrame (`df_elements`) containing elements from a document (e.g., text, formulas, and page breaks) to identify and remove all rows belonging to pages that contain at least one formula.\n",
        "\n",
        "- `pages_with_formulas_indexes`: A `set` to store the indices of pages that contain at least one `Formula`.\n",
        "- `index`: Keeps track of the current page number (starts at 0).\n",
        "- `page_indices`: A list to map each row in the DataFrame to its corresponding page number.\n",
        "\n",
        "\n",
        "In the end, we filter out rows whose `PageIndex` matches any value in `pages_with_formulas_indexes`. The `PageIndex` column is dropped after filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8Rq_yIt4WNR5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of PageBreak elements: 322\n",
            "Number of PageBreak elements: 300\n",
            "Pages with formulas indexes: [89, 94, 95, 96, 97, 136, 142, 157, 160, 165, 168, 180, 181, 182, 184, 187, 189, 193, 194, 203, 208, 210]\n",
            "Number of pages with at least one formula: 22\n"
          ]
        }
      ],
      "source": [
        "# Initial number of PageBreak elements\n",
        "print(\"Number of PageBreak elements: \" + str(len(df_elements[df_elements['Type'] == PageBreak])))\n",
        "\n",
        "\n",
        "# list of page indexes with at least one formula\n",
        "pages_with_formulas_indexes = set()\n",
        "index = 0\n",
        "page_indices = [] # to map each row to its page index\n",
        "\n",
        "for i in range(len(df_elements)):\n",
        "    # when a PageBreak is found, increment the page-index\n",
        "    if df_elements.iloc[i]['Type'] == PageBreak:\n",
        "        index += 1\n",
        "    page_indices.append(index)\n",
        "    # if you find a formula, add the page index to the set\n",
        "    if df_elements.iloc[i]['Type'] == Formula:\n",
        "        pages_with_formulas_indexes.add(index)\n",
        "\n",
        "# add the PageIndex column to the DataFrame\n",
        "df_elements['PageIndex'] = page_indices\n",
        "\n",
        "# filter out the rows with formulas\n",
        "df_elements_cleaned = df_elements[~df_elements['PageIndex'].isin(pages_with_formulas_indexes)].drop(columns=['PageIndex']).reset_index(drop=True)\n",
        "\n",
        "# Final number of PageBreak elements\n",
        "print(\"Number of PageBreak elements: \" + str(len(df_elements_cleaned[df_elements_cleaned['Type'] == PageBreak])))\n",
        "print(\"Pages with formulas indexes:\", sorted(pages_with_formulas_indexes))\n",
        "print(\"Number of pages with at least one formula:\", len(pages_with_formulas_indexes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oPbgAD5jWNR5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of elements before filtering: 4031\n",
            "Number of elements after filtering: 3656\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of elements before filtering:\", len(df_elements))\n",
        "print(\"Number of elements after filtering:\", len(df_elements_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZqXuKB-WNR5"
      },
      "source": [
        "The Unstructured pipeline is not perfect and can make some mistakes in extracting text from the pdf. This code replaces uncommon Unicode characters with ASCII equivalents and removes any remaining non-ASCII characters. We defined a dictionary for replacing specific Unicode characters (e.g., `ﬃ → ffi`, `⊺ → T`), more can be added if necessary. We use `str.maketrans()` for fast replacements. The computed values are stored back in the `Value` column of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T9wSR0CQWNR5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "unicode_replacements = {\n",
        "    \"ﬃ\": \"ffi\",\n",
        "    \"ﬀ\": \"ff\",\n",
        "    \"ﬁ\": \"fi\",\n",
        "    \"⊺\": \"T\",\n",
        "    \"•\": \" \",\n",
        "    \"—\": \"-\",\n",
        "    \"«\": \" \",\n",
        "    \"»\": \" \",\n",
        "    \"”\": '\"',\n",
        "    \"“\": '\"',\n",
        "    \"‘\": \"'\",\n",
        "    \"’\": \"'\",\n",
        "    \"ﬄ\": \"ffl\",\n",
        "    # more can be added if needed!\n",
        "}\n",
        "\n",
        "# translation table for faster replacement\n",
        "translation_table = str.maketrans(unicode_replacements)\n",
        "\n",
        "def find_non_ascii_characters(text):\n",
        "    # replace all characters using the translation table\n",
        "    text = text.translate(translation_table)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]', '', text)  # Removes all remaining non-ASCII characters\n",
        "    return text\n",
        "\n",
        "# Apply the function to the 'Value' column\n",
        "df_elements_cleaned['Value'] = df_elements_cleaned['Value'].apply(find_non_ascii_characters)\n",
        "\n",
        "df_elements = df_elements_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0oMEuCOT0H2"
      },
      "source": [
        "## Nougat: Neural Optical Understanding for Academic Documents\n",
        "\n",
        "\n",
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "    .center {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "    }\n",
        "    .center img {\n",
        "        width: 15%;\n",
        "    }\n",
        "</style>\n",
        "<div class=\"center\">\n",
        "        <img src=\"assets/meta-emblem.png\" alt=\"meta-emblem\">\n",
        "</div>\n",
        "\n",
        "\n",
        "[**Nougat**](https://github.com/facebookresearch/nougat?tab=readme-ov-file) is an academic document PDF parser designed to extract and understand LaTeX math and tables from PDFs. Developed by **Facebook Research**, it converts academic documents into structured formats (Mathpix Markdown, *.MMD*).\n",
        "\n",
        "Key Features:\n",
        "- Extracts and processes LaTeX math, tables, and text.\n",
        "- Converts PDFs to Mathpix Markdown (.mmd) format.\n",
        "- Supports GPU for faster processing.\n",
        "\n",
        "Nougat is optimized for scientific papers, especially those found on platforms like arXiv, and works best with English papers.\n",
        "\n",
        "The approach for this project is to leverage Nougat as a *targeted solution* for extracting content from pages that present particular challenges, such as those containing mathematical fomulas, on which Unstructured performed poorly.\n",
        "\n",
        "> *But why wasn't `Nougat` applied to the entire PDF?* The reason is that the selected PDF is over 300 pages long, and processing it with Nougat for the entire document would have been time-consuming. Instead, we used `Unstructured` to identify the pages containing formulas and applied `Nougat` only to those problematic pages. This resulted in a faster and more efficient overall workflow, while maintaining accuracy.\n",
        "\n",
        "> As a side note, unfortunately Nougat requires `transformers==4.38.2` or below, which is uncompatible with `sentence-transformers`, since it requires `transformers<5.0.0,>=4.41.0`. A solution to this is to keep them separated in two different virutal environments. For the sake of semplicity, in the notebook we will simply upgrade `transformers` later on in order to utilize `sentence-transformers` without any problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kmq5mVmfWNR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nougat-ocr 0.1.17 requires timm==0.5.4, but you have timm 1.0.12 which is incompatible.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "effdet 0.4.1 requires timm>=0.9.2, but you have timm 0.5.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# install some pre-requisites for Nougat\n",
        "%pip install -qqq -U albumentations\n",
        "%pip -qqq install transformers==4.38.2 --progress-bar off\n",
        "\n",
        "# install Nougat\n",
        "%pip -qqq install nougat-ocr  --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJEnWwFdWNR6"
      },
      "source": [
        "Now we can simply invoke Nougat, by calling the corresponding CLI command. More information about it can be found on its [README page](https://github.com/facebookresearch/nougat?tab=readme-ov-file). Here it is called by simply passing:\n",
        "- the path of the PDF file in analysis\n",
        "- `-o`, the output folder\n",
        "- `--pages`, to specify only pages containing mathemathical formulas\n",
        "- `-m 0.1.0-base`, to utilize the base model (default to `0.1.0-small` but didn't produce acceptable results)\n",
        "- `--no-skipping`, to avoid using failure detection heuristic (this was added because, as from the Nougat readme, *'on some devices the failure detection heuristic are not working properly'*, and it is suggested to run it with this flag)\n",
        "\n",
        "This step requires approximately from 2 to 5 minutes.\n",
        "\n",
        ">  NOTE : If the file already exists, the command **WON'T** be re-executed. Call Nougat again and pass `--recompute` to ignore this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ewz8syhoWNR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90,95,96,97,98,137,143,158,161,166,169,181,182,183,185,188,190,194,195,204,209,211\n",
            "nougat \"Information Retrieval Slides.pdf\" -o ./nougat-output --pages 90,95,96,97,98,137,143,158,161,166,169,181,182,183,185,188,190,194,195,204,209,211 -m 0.1.0-base --no-skipping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\giuli\\miniconda3\\envs\\lm\\Lib\\site-packages\\nougat\\transforms.py:146: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
            "  alb.ElasticTransform(\n",
            "C:\\Users\\giuli\\miniconda3\\envs\\lm\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "INFO:root:Skipping Information Retrieval Slides.pdf, already computed. Run with --recompute to convert again.\n"
          ]
        }
      ],
      "source": [
        "# Nougat is 1-indexed, so we need to increment the page indexes by 1\n",
        "nougat_formulas_indexes = [index + 1 for index in pages_with_formulas_indexes]\n",
        "\n",
        "nougat_formulas_indexes_str = \",\".join(map(str, sorted(nougat_formulas_indexes)))\n",
        "print(nougat_formulas_indexes_str)\n",
        "\n",
        "command = f'nougat \"{PDF_NAME}\" -o ./nougat-output --pages {nougat_formulas_indexes_str} -m 0.1.0-base --no-skipping'\n",
        "\n",
        "print(command)\n",
        "\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs4Ea2tIWNR7"
      },
      "source": [
        "The output can be found in the `nougat-output` folder, as an MMD file. Since of course the model can fail to identify some sentences, we also perform some preprocessing of the text, to ensure it will be sanitized for the next phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FLhsgB3aWNR7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard Coefficient\n",
            "\n",
            "* A common measure of overlap of two (finite) sets \\(A\\) and \\(B\\): \\[J(A,B)=\\frac{|A\\cap B\\,|}{|A\\cup B\\,|}\\]\n",
            "* Always assigns a number between 0 and 1\n",
            "* \\(A\\) and \\(B\\) do not h  ...\n",
            "\n",
            "..........................................\n"
          ]
        }
      ],
      "source": [
        "def remove_unmatched_left_tags(line):\n",
        "        if \"\\\\left|\" in line and \"\\\\right|\" not in line:\n",
        "            line = line.replace(\"\\\\left|\", \"\")\n",
        "        return line\n",
        "\n",
        "def preprocess_content(content):\n",
        "    content = content.replace(\"**\", \"\").replace(\"##\", \"\")  # Some unuseful markdown is inserted sometimes\n",
        "    content = content.replace(\"{(}\", \"(\").replace(\"{)}\", \")\") # replace all '{(}' and '{)}' with '(' and ')'\n",
        "    content = content.replace(\"{[}\", \"(\").replace(\"{]}\", \")\") # replace all '{[}' and '{]}' with '(' and ')'\n",
        "    content = re.sub(r'\\$(.*?)\\$', r'\\\\(\\1\\\\)', content) # replace all '$' with '\\(\\)' (inline formulas)\n",
        "    content = re.sub(r'\\\\text\\{(.*?)\\}', r'\\1', content) # replace all '\\text' with ''\n",
        "    content = re.sub(r'_(.*?)_', r'\\1', content) # replace words inside '_word_' with the same word -> e.g. _Proof:_ -> Proof:\n",
        "    content = re.sub(r'\\\\includegraphics\\[width=.*?pt\\]', '', content) # replace all \\includegraphics[width=....pt] with '', independently of the width\n",
        "    content = \"\\n\".join(remove_unmatched_left_tags(line) for line in content.splitlines()) # remove all \\left|\\ tags that don't have a \\right|\\ tag in the same line (they are not closed)        \n",
        "    content = content.replace('\\\\\\\\', '\\\\') # substitute '\\\\' with '\\', since it's not necessary to escape the backslash for later\n",
        "    content = content.strip()\n",
        "    return content\n",
        "\n",
        "\n",
        "with open(f'nougat-output/{PDF_NAME.replace(\".pdf\",\".mmd\")}', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "\n",
        "processed_content = preprocess_content(content)\n",
        "\n",
        "\n",
        "with open(f'nougat-output/{PDF_NAME.replace(\".pdf\",\".mmd\")}', 'w') as f:\n",
        "    f.write(processed_content)\n",
        "\n",
        "\n",
        "print(processed_content[:200], \" ...\\n\\n..........................................\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1-7NgCVMY9"
      },
      "source": [
        "# Creating the chunks\n",
        "\n",
        "Before proceeding, we clean up the `df_elements` DataFrame by removing consecutive duplicate rows while retaining the first occurrence. This issue was present sometimes in our case because Unstructured sometimes duplicated Page Titles, or somtimes because the PDF contained white-on-white text, which resulted in duplicate content for the same page. Cleaning the DataFrame ensures to avoid redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KBJg2MZ3QH1t"
      },
      "outputs": [],
      "source": [
        "# drop adjacent duplicates in the df_elements mantaining the first one\n",
        "df_elements = df_elements.loc[df_elements.shift().ne(df_elements).any(axis=1)]\n",
        "\n",
        "# reset index\n",
        "df_elements = df_elements.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g_33-bMTulyW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final number of pages (given by the number of PageBreak(s)): 300\n"
          ]
        }
      ],
      "source": [
        "print(\"Final number of pages (given by the number of PageBreak(s)):\",len(df_elements[df_elements['Type'] == PageBreak]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenization is the process of breaking a text into smaller units, such as words or phrases, which is a fundamental step in text preprocessing for natural language processing (NLP) tasks. We use the  **Natural Language Toolkit (NLTK)** for this task and counting tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8vMk0-oV-_1B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\giuli\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def count_tokens(text):\n",
        "  return len(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v598nbR51juB"
      },
      "source": [
        "Here we compute some statistics regarding the number of tokens in the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HpV8q_skMHGl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max tokens: 558\n",
            "Min tokens: 1\n",
            "Avg tokens: 68.28\n"
          ]
        }
      ],
      "source": [
        "# compute the max, the min and the average number of tokens in a page by concatenating elements of the dataframe between consecutive page breaks\n",
        "\n",
        "num_token_per_page = []\n",
        "\n",
        "current_page = \"\"\n",
        "current_page_tokens = 0\n",
        "\n",
        "for i in range(len(df_elements)):\n",
        "  if df_elements.iloc[i]['Type'] != PageBreak:\n",
        "    current_page += df_elements.iloc[i]['Value']\n",
        "    current_page_tokens = count_tokens(current_page)\n",
        "  else:\n",
        "    num_token_per_page.append(current_page_tokens)\n",
        "    current_page = \"\"\n",
        "    current_page_tokens = 0\n",
        "\n",
        "print(\"Max tokens: \" + str(max(num_token_per_page)))\n",
        "print(\"Min tokens: \" + str(min(num_token_per_page)))\n",
        "print(\"Avg tokens: \" + str(sum(num_token_per_page)/len(num_token_per_page)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8SttzqQSMhxG"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mean_tokens = np.mean(num_token_per_page)\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.hist(num_token_per_page, bins=10, edgecolor='black', alpha=0.5)\n",
        "plt.axvline(mean_tokens, color='red', linestyle='dashed', linewidth=1, label=f'Mean = {mean_tokens:.2f}')\n",
        "plt.title('Distribution of Number of Tokens per Page')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE2y3MZgVuIa"
      },
      "source": [
        "# Splitting Content into Tokenized Chunks\n",
        "\n",
        "We can now divide the processed elements in `df_elements` into smaller text files, ensuring that each file stays within a token limit.\n",
        "\n",
        "The following code is designed to split the content into manageable chunks while maintaining logical consistency and handling page breaks effectively.\n",
        "\n",
        "Two key parameters are defined for efficient chunking:\n",
        "- **`doc_tokens_threshold`**: The maximum number of tokens allowed per chunk.\n",
        "- **`overlap_tokens`**: The number of tokens to overlap between consecutive chunks, which helps maintain coherence between chunks.\n",
        "\n",
        "These parameters are document-dependent, and experimenting with different values can help achieve better results depending on the structure and complexity of the document.\n",
        "\n",
        "### **Processing and Chunking the Content:**\n",
        "The main loop processes the `df_elements` dataframe, splitting the content based on the following rules:\n",
        "- If the next element would cause the chunk to exceed the token limit, the current chunk is saved, and the next chunk starts with a portion of the previous one (based on the overlap).\n",
        "- When encountering a `PageBreak`, the content is saved, and the next block starts on the new page, again with an overlap.\n",
        "\n",
        "This process ensures that:\n",
        "- The token limit is respected for each chunk.\n",
        "- The context is preserved by maintaining overlap between chunks.\n",
        "- The integrity of formulas and page transitions is respected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H6CxZzdOEqnJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks obtained: 103\n"
          ]
        }
      ],
      "source": [
        "def save_chunk(content, index):\n",
        "    with open(f'chunks/document_{index}.mmd', 'w') as f:\n",
        "        f.write(content)\n",
        "    docs_len.append(count_tokens(content))\n",
        "    return index + 1\n",
        "\n",
        "\n",
        "\n",
        "# Create a directory to store the chunks\n",
        "if not os.path.exists('chunks'):\n",
        "    os.makedirs('chunks')\n",
        "else:\n",
        "    for file in os.listdir('chunks'):\n",
        "        os.remove(os.path.join('chunks', file))\n",
        "\n",
        "block = \"\"\n",
        "document = \"\"\n",
        "doc_index = 0\n",
        "doc_tokens_threshold = 300\n",
        "overlap_tokens = 30\n",
        "docs_len = []\n",
        "\n",
        "# Iterate over the elements of the dataframe\n",
        "for i in range(len(df_elements)):\n",
        "    element_type = df_elements.at[i, 'Type']\n",
        "    next_element = df_elements.at[i, 'Value'] + \" \"\n",
        "\n",
        "    if element_type != PageBreak:\n",
        "        if count_tokens(block) + count_tokens(next_element) > doc_tokens_threshold:\n",
        "            # print(\"block overlap happened for :\", doc_index)\n",
        "            doc_index = save_chunk(block, doc_index)\n",
        "            overlap = block.split()[-overlap_tokens:]\n",
        "            block = \" \".join(overlap) + \" \" + next_element\n",
        "        else:\n",
        "            block += next_element\n",
        "    else:\n",
        "        if block:  # if there is content in the block\n",
        "            if document:  # if there is content in the document\n",
        "                if count_tokens(document) + count_tokens(block) > doc_tokens_threshold:\n",
        "                    doc_index = save_chunk(document, doc_index)\n",
        "                    # print(\"doc overlap happened for :\", doc_index)\n",
        "                    # maintain overal also for pages\n",
        "                    overlap = document.split()[-overlap_tokens:]\n",
        "                    document = \" \".join(overlap) + \" \" + block\n",
        "                else:\n",
        "                    document += block\n",
        "            else:\n",
        "                document = block\n",
        "            block = \"\"\n",
        "\n",
        "# handling last block\n",
        "if block:\n",
        "    if document:\n",
        "        if count_tokens(document) + count_tokens(block) > doc_tokens_threshold:\n",
        "            doc_index = save_chunk(document, doc_index)\n",
        "            doc_index = save_chunk(block, doc_index)\n",
        "        else:\n",
        "            doc_index = save_chunk(document + block, doc_index)\n",
        "    else:\n",
        "        doc_index = save_chunk(block, doc_index)\n",
        "elif document:\n",
        "    doc_index = save_chunk(document, doc_index)\n",
        "\n",
        "print(\"Number of chunks obtained: \" + str(doc_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up to now, we have not used the output resulted by **Nougat**: for the `Information Retrieval Slides.mmd` file produced, we apply a custom approach.\n",
        "\n",
        "We use the **`CharacterTextSplitter`** from LangChain to split the text into chunks, as the content is continuous and requires a different processing strategy compared to documents with page breaks.\n",
        "\n",
        "Again, `chunk_size` and `chunk_overlap` are document specific values, and changing them may produce better results depending on the type of document to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks obtained: 117\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\", chunk_size=500, chunk_overlap=50\n",
        ")\n",
        "texts = text_splitter.split_text(processed_content)\n",
        "\n",
        "for t in texts:\n",
        "    doc_index = save_chunk(t, doc_index)\n",
        "\n",
        "print(\"Number of chunks obtained: \" + str(doc_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mwVWd3HhzOov"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents:  117\n",
            "Maximum number of tokens in a document:  546\n",
            "Minimum number of tokens in a document:  137\n",
            "Average number of tokens in a document:  268.8888888888889\n"
          ]
        }
      ],
      "source": [
        "# print the max, the min and the avg of values in docs_len\n",
        "print(\"Number of documents: \",len(docs_len))\n",
        "print(\"Maximum number of tokens in a document: \",max(docs_len))\n",
        "print(\"Minimum number of tokens in a document: \",min(docs_len))\n",
        "print(\"Average number of tokens in a document: \",sum(docs_len)/len(docs_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5j3ay1zTLO5"
      },
      "source": [
        "# RAG Pipeline on PDFs with Limited GPU Requirements\n",
        "\n",
        "Now, our aim is to build a **Retrieval-Augmented Generation (RAG)** pipeline optimized for a **limited GPU environments**. The goal is to combine the power of *dense retrieval-based methods* with the flexibility of generative models, while ensuring the system remains efficient enough to run on medium-tier laptops with GPUs.\n",
        "\n",
        "As said in the introduction, a RAG pipeline consists of two main stages: **retrieval** and **generation**. The entire process can be divided into **3 key steps**:\n",
        "\n",
        "1. **Generation of document embeddings**  \n",
        "   In this step, document embeddings are generated from a *corpus* using an embedding model. Each document is encoded into a dense vector representation, capturing semantic information.\n",
        "\n",
        "2. **Document retrieval**  \n",
        "   Relevant documents are fetched from the corpus based on the input query. This is achieved by utilizing the generated embeddings and calculating similarity with the query's embedding to retrieve the most relevant documents.\n",
        "\n",
        "3. **Output generation**  \n",
        "   The retrieved documents are then passed to a **generative model**, which produces contextually relevant responses based on the information extracted from the documents.\n",
        "\n",
        "By leveraging lightweight models and optimizing for efficiency, this RAG pipeline should be able to deliver good results even on hardware with **limited resources**.\n",
        "\n",
        "Finally, a **Gradio interface** is provided, allowing users to interact with the system and \"chat\" with the documents they provide, offering a seamless experience for exploring the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzLXqhNeTLO9"
      },
      "source": [
        "## Hardware requirements and constraints\n",
        "\n",
        "Let's find out what hardware we've got available to see what kind of model(s) we'll be able to load. You can also check this with the `!nvidia-smi` command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmd7W4v-WNSD"
      },
      "source": [
        "> **NOTE** : as said above, `sentence-transformers` requires a different version of the `transformers` library, so we update it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-Yd9LgCsWNSE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qqq -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fkHZDXjWNSE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq99vywlTLO-"
      },
      "outputs": [],
      "source": [
        "# Get GPU available memory\n",
        "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN4Pji5BTLO-"
      },
      "source": [
        "Of course, depending on the provided harware, better models can be utilized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMiWvBYzTLO-"
      },
      "source": [
        "### Checking local GPU memory availability\n",
        "\n",
        "Let's first analyze how we decided the model to use for this project. This notebook was primarily run and tested locally on a **laptop** with 16GB of RAM and an NVIDIA RTX 3070 laptop GPU (8GB of VRAM). The main goal was to create a pipeline that could efficiently run on this portable device, leveraging the benefits of GPUs and CUDA for AI tasks while ensuring that the performance and capabilities did not feel lacking compared to larger models.\n",
        "\n",
        "We will need two main ingredients:\n",
        "- An *embedder* model, that calculates dense embeddings from documents\n",
        "- An *LLM*, that provides output given user's queries and the retrieved documents\n",
        "\n",
        "In their dedicated sections we will uncover the choiches made for both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPrgoawNTLO_"
      },
      "source": [
        "### Load our PDF and start producing chunks\n",
        "\n",
        "Let's now start by loading a pdf file and extracting chunks from it. These chunks' **quality** is important, since these will be essentially the *documents* on which we will compute the embeddings on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBaOxo4gaimg"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "PDF_NAME = \"Information Retrieval Slides.pdf\"\n",
        "running_on_colab = False\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata # type: ignore\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Running in Google Colab. Using userdata to get HF_TOKEN.\")\n",
        "    running_on_colab = True\n",
        "except ModuleNotFoundError:\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "    print(\"Not running in Google Colab. Using load_dotenv to get HF_TOKEN.\")\n",
        "\n",
        "file_path = PDF_NAME\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5jUKmVnUnI-"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "# get files that are named 'document_<number>.mmd'\n",
        "files = sorted([f for f in os.listdir(\"chunks\") if f.startswith(\"document_\") and f.endswith(\".mmd\")],\n",
        "    key=lambda x: int(x.split(\"_\")[1].split(\".\")[0])  # order numerically by the number in the filename\n",
        ")\n",
        "\n",
        "for file in files:\n",
        "    with open(os.path.join(\"chunks\", file), \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus.append(f.read())\n",
        "\n",
        "print(f\"Number of documents loaded: {len(corpus)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBAwgyvTLPA"
      },
      "source": [
        "Let's give a look to some random documents extracted and preprocessed from the initial pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKx35jopaimh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(4242)\n",
        "\n",
        "# print some random pages with their indices\n",
        "for i in range(5):\n",
        "    doc = random.choice(corpus)\n",
        "    doc_index = corpus.index(doc)\n",
        "    print(f\"DocId: {doc_index}, {doc.replace(\"\\n\", \" \")[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdh2ECCcaimi"
      },
      "outputs": [],
      "source": [
        "print(f\"Average document length: {sum(len(doc) for doc in corpus) / len(corpus)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhCC7pboWNSF"
      },
      "source": [
        "Later, we will use a `json` files containing questions to test the pipeline: it is made up by 69 open-questions we prepared on the PDF topic, in order to asses the final RAG capabilities. For now, we’ll download this file and use it also in this paragraph to ensure the pipeline works as expected.\n",
        "\n",
        "We’ll begin by defining the function `extract_questions(file_path)`, which takes as parameter:\n",
        "\n",
        "- `file_path`: the path to the JSON file containing the questions.\n",
        "\n",
        "At this stage, since our primary goal is to verify that everything functions as intended, we'll just use **some** of the proposed questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR2e6XDsaimi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def extract_questions(file_path):\n",
        "    \"\"\"\n",
        "    Extracts questions from a JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        return [\n",
        "            {\n",
        "                \"question_id\": item.get(\"question_id\", \"\"),\n",
        "                \"question\": item.get(\"question\", \"\")\n",
        "            }\n",
        "            for item in data\n",
        "        ]\n",
        "\n",
        "\n",
        "if not os.path.exists(\"evaluation\"):\n",
        "    os.makedirs(\"evaluation\")\n",
        "    # save locally from https://drive.google.com/file/d/1m2_iG7cGOgRwfVaXzTORU7Pcn71UFKcG/view?usp=drive_link\n",
        "    gdown.download(id=\"1m2_iG7cGOgRwfVaXzTORU7Pcn71UFKcG\", output=\"evaluation/quiz.json\", quiet=False)\n",
        "    # save locally from https://drive.google.com/file/d/1L6rvrbPtwGduaN-8DmpLAWXUtHSfhhF2/view?usp=sharing\n",
        "    gdown.download(id=\"1L6rvrbPtwGduaN-8DmpLAWXUtHSfhhF2\", output=\"evaluation/open_questions.json\", quiet=False)\n",
        "\n",
        "queries = extract_questions(\"evaluation/open_questions.json\")\n",
        "print(f\"Loaded {len(queries)} questions.\")\n",
        "\n",
        "random.seed(4242)\n",
        "print(random.choice(queries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPIvk-TUTLPB"
      },
      "source": [
        "## Gather Dataset Embeddings\n",
        "\n",
        "In this first stage, the aim is to produce the documents embeddings, in order to be able to make similarity searches in the upcoming steps of the pipeline.\n",
        "\n",
        "### About the `cde-small-v1` Model\n",
        "\n",
        "The `cde-small-v1` model, developed by John X. Morris and Alexander M. Rush, is a cutting-edge model for generating **Contextual Document Embeddings (CDE)** ([link](https://huggingface.co/jxm/cde-small-v1)). What sets this model apart is its ability to integrate \"*context tokens*\" into the embedding process, which allows it to capture the nuances and relationships between documents more effectively. This makes it particularly suitable for generating highly accurate embeddings for both documents and queries, especially in cases where capturing the context of a document within the broader corpus is crucial.\n",
        "\n",
        "We chose this model because, as of December 2024, it is one of the leading models under 400M parameters, delivering impressive results on the [**Massive Text Embedding Benchmark (MTEB) leaderboard**](https://huggingface.co/spaces/mteb/leaderboard). Although it ranks 29th overall, it stands out as the top model in terms of **memory efficiency**, which is a key factor for our project, given the requirement for **limited GPU capabilities**. Additionally, it offers a substantial **embedding dimension of 768**, striking a balance between computational efficiency and embedding quality.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" alt=\"HF logo\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sg3xXhiaimk"
      },
      "source": [
        "<style>\n",
        "    img {\n",
        "        border-radius: 15px;\n",
        "    }\n",
        "</style>\n",
        "![assets/cde-small-v1.png](assets/cde-small-v1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Features of Contextual Embeddings\n",
        "Traditional embedding models treat documents as independent entities, which can miss subtle but important relationships within a corpus. The `cde-small-v1` model solves this by incorporating contextual embeddings, where a document's representation is enriched with information from related documents.\n",
        "\n",
        "This is achieved by:\n",
        "\n",
        "- Clustering Neighboring Documents: Documents are grouped into clusters based on their similarity. For a document , its context is derived from nearby documents in the embedding space.\n",
        "- Integrating Context: The model combines the target document's standalone embedding with the aggregated representation of its cluster to produce a final, enriched embedding.\n",
        "This process ensures that embeddings are not only descriptive of individual documents but also reflective of their relationships within the corpus.\n",
        "\n",
        "In parrticular , to train the `cde-small-v1` model, contrastive learning is used. This method ensures that similar documents are placed closer in the embedding space while dissimilar documents are pushed farther apart.\n",
        "\n",
        "Basically documents from the same cluster or context are treated as similar pairs and dissimilar documents (hard negatives) are included to make the training process more effective and robust\n",
        "\n",
        "During training, the model optimizes a loss function that balances these relationships:\n",
        "- Positive Pair: Maximizes similarity between a query and a relevant document.\n",
        "- Negative Pair: Minimizes similarity between a query and irrelevant documents.\n",
        "\n",
        "This approach should leads to embeddings that are capable of accurately distinguishing between relevant and irrelevant documents, even in complex domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqbzIDgTLPC"
      },
      "source": [
        "A key feature of the `cde-small-v1` model is its optimization for a two-stage embedding process:\n",
        "\n",
        "1. **First Stage**: Embedding a subset of documents from the corpus to create \"dataset embeddings,\" which serve as a **reference** for the entire corpus.\n",
        "2. **Second Stage**: Using the dataset embeddings to embed new queries and documents during inference.\n",
        "\n",
        "This model is compact yet delivers solid performance, making it suitable for our use case.\n",
        "\n",
        "### Steps to Gather Dataset Embeddings:\n",
        "\n",
        "1. **Selecting a Subset of Documents**  \n",
        "   We begin by sampling a representative set of documents from the corpus. Following the model's guidelines, we select 512 documents. If this number isn't available, the model can handle oversampling, which is the case for the PDF dataset we're using. Despite this, performance remains strong.\n",
        "\n",
        "2. **Generating Dataset Embeddings**  \n",
        "   After selecting the documents, we encode them using the `cde-small-v1` embedding model. This step produces dense vector representations of the documents, which are representative of the broader corpus.\n",
        "\n",
        "3. **Embedding Queries and Documents**  \n",
        "   Once the dataset embeddings are created, we use them to embed both documents and queries. A key feature of this model is its ability to differentiate between 'queries' and 'documents' during encoding, ensuring context is preserved during the embedding process.\n",
        "\n",
        "\n",
        "Next, we will load the model using the `SentenceTransformers` interface to begin the embedding process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eW5vIiUTLPC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# with sentence-transformers we don't need prefixes but, to do retrieval, we need to use prompt_name=\"query\" and prompt_name=\"document\" in the encode method of the model when embedding queries and documents, respectively.\n",
        "embeddings_model = SentenceTransformer(\n",
        "    \"jxm/cde-small-v1\",\n",
        "    trust_remote_code=True,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W7Dz05MTLPC"
      },
      "source": [
        "During the development of the project, the creators of the embedding model made a change in the Hugging Face repository, which led to unstable results when using the `SentenceTransformer` implementation. If a similar issue arises in the future, it can be mitigated by specifying the `revision` and `tokenizer_kwargs` parameters in the model constructor. These parameters allow to lock the model and tokenizer to a specific branch name, tag, or commit ID from the Hugging Face repository, ensuring stability.\n",
        "\n",
        "Here’s how to implement it:\n",
        "\n",
        "```python\n",
        "embeddings_model = SentenceTransformer(\n",
        "    \"jxm/cde-small-v1\",\n",
        "    trust_remote_code=True,\n",
        "    revision=\"9e2ed1d8d569d34458913d2d246935c1b2324d11\",  # Latest stable model revision\n",
        "    tokenizer_kwargs={\"revision\": \"86b5e0934494bd15c9632b12f734a8a67f723594\"}  # Latest stable tokenizer revision\n",
        ").to(device)\n",
        "```\n",
        "\n",
        "The tags provided above correspond to the latest stable commits (as of December 2024). You can retrieve them directly from the model's card page on Hugging Face, under the \"Files and Versions\" tab.\n",
        "\n",
        "Fortunately for us, the issue was resolved the same day we reported it to the creators via Twitter. We thank them for their prompt response and the fix they provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvDHtnRDTLPC"
      },
      "source": [
        "Let's follow up by producing the `minicorpus` (which is the subsample of the whole corpus) and what we called `dataset_embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ISVKFnGaimk"
      },
      "outputs": [],
      "source": [
        "minicorpus_size = embeddings_model[0].config.transductive_corpus_size # 512\n",
        "random.seed(4242)\n",
        "minicorpus_docs = random.choices(corpus, k=minicorpus_size) # oversampling is okay\n",
        "assert len(minicorpus_docs) == minicorpus_size # We must use exactly this many documents in the minicorpus\n",
        "\n",
        "dataset_embeddings = embeddings_model.encode(\n",
        "    [doc for doc in minicorpus_docs],\n",
        "    prompt_name=\"document\",\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(f\"Corpus size: {len(corpus)}\")\n",
        "print(f\"Computed embeddings for {len(minicorpus_docs)} documents. Shape: {dataset_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWDiKrp7aiml"
      },
      "outputs": [],
      "source": [
        "print(\"Some mini-corpus documents:\")\n",
        "\n",
        "# get some random documents from the minicorpus\n",
        "random.seed(42)\n",
        "for i in random.sample(range(minicorpus_size), 5):\n",
        "    print(f\"document {i}: {minicorpus_docs[i].replace(\"\\n\", \" \")[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLI5qjGaiml"
      },
      "source": [
        "Now that we have obtained the **dataset embeddings**, we can proceed to embed both documents and queries using the same model.\n",
        "\n",
        "To embed the documents and queries, we must ensure that we specify the correct `prompt_name` for each, as well as pass the `dataset_embeddings` to maintain context. The reason is that, as many state-of-the-art-models, this one was trained with task-specific prefixes:\n",
        "\n",
        "- For documents, use:  \n",
        "  `prompt_name=\"document\"`\n",
        "\n",
        "- For queries, use:  \n",
        "  `prompt_name=\"query\"`\n",
        "\n",
        "We also have to always additionally, specify  the `dataset_embeddings`, in order to use the once we produced before.\n",
        "\n",
        "By doing so, we ensure that the embeddings are generated with the correct context for both retrieval and generation tasks, leveraging the efficient performance of the `cde-small-v1` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-4WdCtLaiml"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = embeddings_model.encode(\n",
        "    [doc for doc in corpus],\n",
        "    prompt_name=\"document\",\n",
        "    dataset_embeddings=dataset_embeddings, # this is the contexualized embeddings of the minicorpus\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "queries_embeddings = embeddings_model.encode(\n",
        "    [query['question'] for query in queries],\n",
        "    prompt_name=\"query\",\n",
        "    dataset_embeddings=dataset_embeddings,  # this is the contexualized embeddings of the minicorpus\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
        "print(f\"Query embeddings shape: {queries_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQx9Lr3TLPD"
      },
      "source": [
        "We can now computes similarities between the embeddings and all the queries by simply calling `embeddings_model.similarity` (which uses by default cosine similarity), and inspect some of the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8coT3HV9aiml"
      },
      "outputs": [],
      "source": [
        "similarities = embeddings_model.similarity(queries_embeddings, doc_embeddings)\n",
        "print(\"similarities shape: \",similarities.shape)\n",
        "topk_values, topk_indices = similarities.topk(5)\n",
        "\n",
        "print(\"topk_values (scores) shape: \",topk_values.shape, \"\\ntopk_indices (doc_ids) shape: \",topk_indices.shape,\"\\n\") # both are made up of tensors of shape (num_queries, k)\n",
        "\n",
        "random.seed(4242)\n",
        "random_queries = random.sample(queries, 2)\n",
        "for query in random_queries:\n",
        "    query_idx = queries.index(query)\n",
        "    print(f\"Query: {query['question']}\")\n",
        "    for j, idx in enumerate(topk_indices[query_idx]):\n",
        "        doc = corpus[idx].replace(\"\\n\", \" \")\n",
        "        print(f\"Rank {j+1} (Score: {topk_values[query_idx][j]:.4f}, Doc ID: {idx}): {doc}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTwJIF5aimn"
      },
      "source": [
        "### Compute scores between queries and documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp-7zfI3TLPJ"
      },
      "source": [
        "Now we can analyze a similarity heatmap between the embeddings of our documents and the provided queries. The results show consistency with the following observations:\n",
        "\n",
        "- **Few documents match the queries**: This suggests that the matching documents are likely the most relevant ones.\n",
        "- **Matches often occur in subsequent documents**: This is expected since the corpus was split into chunks based on the document's structure. For example, chapters are separated, so answers to specific questions are more likely to be located within the same chapter or adjacent sections of the document.\n",
        "\n",
        "Additionally, we provide a distribution of the similarity scores. This helps us assess whether an empirical threshold can be established to determine which documents are truly similar to the provided queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4zQUCwaaimn"
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot heatmap\n",
        "heatmap(similarities.cpu().numpy().T, cmap=\"jet\", ax=axes[0])\n",
        "axes[0].set_title(\"Similarity Heatmap\")\n",
        "\n",
        "# Plot histogram\n",
        "axes[1].hist(similarities.cpu().flatten(), bins=50, color='blue', alpha=0.6)\n",
        "axes[1].set_title(\"Distribution of Similarity Scores\")\n",
        "axes[1].set_xlabel(\"Similarity\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].axvline(x=0.5, color='red', alpha=0.6, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "miniqueries = queries[:5]\n",
        "print(miniqueries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ir_measures import *\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "def load_qrels(qrels_path):\n",
        "    \"\"\"\n",
        "    Load qrels from a txt tab separated file.\n",
        "    Expected format: \n",
        "    query_id document_id relevance iteration\n",
        "    \"\"\"\n",
        "    qrels = defaultdict(dict)\n",
        "    with open(qrels_path, 'r') as file:\n",
        "        for line in file:\n",
        "            query_id, doc_id, relevance, _ = line.strip().split()\n",
        "            qrels[query_id][doc_id] = int(relevance)\n",
        "    return dict(qrels)\n",
        "\n",
        "\n",
        "def evaluate_bm25_retrieval(queries, qrels, k=10, corpus=None):\n",
        "    if corpus is None:\n",
        "        raise ValueError(\"Corpus is not defined.\")\n",
        "    \n",
        "    # Tokenize documents for BM25\n",
        "    tokenized_docs = [doc.lower().split() for doc in corpus]\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    run = defaultdict(dict)\n",
        "    \n",
        "    for query in tqdm(queries, desc=\"Evaluating BM25 queries\"):\n",
        "        query_id = query['question_id']\n",
        "        doc_scores = bm25.get_scores(query['question'].lower().split())\n",
        "        \n",
        "        # Get top k scores and their indices\n",
        "        top_k_indices = np.argsort(doc_scores)[-k:][::-1]\n",
        "        \n",
        "        # Store scores using document index as doc_id\n",
        "        for idx in top_k_indices:\n",
        "            run[query_id][str(idx)] = doc_scores[idx]\n",
        "\n",
        "    # Save results to file\n",
        "    with open(\"bm25_run.tsv\", \"w\") as f:\n",
        "        for query_id, doc_scores in run.items():\n",
        "            for doc_id, score in doc_scores.items():\n",
        "                f.write(f\"{query_id}\\t{doc_id}\\t{score}\\n\")\n",
        "    \n",
        "    return calculate_metrics(qrels, run)\n",
        "\n",
        "\n",
        "def evaluate_embedding_retrieval(queries, qrels, k=10):\n",
        "        \n",
        "    run = defaultdict(dict)\n",
        "\n",
        "    for query in tqdm(queries, desc=\"Evaluating embedding queries\"):\n",
        "        query_id = query['question_id']\n",
        "        \n",
        "        query_embedding = embeddings_model.encode(\n",
        "            query['question'],\n",
        "            prompt_name=\"query\",\n",
        "            dataset_embeddings=dataset_embeddings,  # this is the contexualized embeddings of the minicorpus\n",
        "            convert_to_tensor=True\n",
        "        )\n",
        "         \n",
        "        similarities = embeddings_model.similarity(query_embedding, doc_embeddings)\n",
        "        topk_values, topk_indices = similarities.topk(k)\n",
        "        topk_indices = topk_indices.cpu().numpy()[0]\n",
        "        topk_values = topk_values.cpu().numpy()[0]\n",
        "        \n",
        "        for doc_idx, score in zip(topk_indices, topk_values):\n",
        "            run[query_id][str(doc_idx)] = float(score)\n",
        "    \n",
        "    with open(\"embedding_run.tsv\", \"w\") as f:\n",
        "        for query_id, doc_scores in run.items():\n",
        "            for doc_id, score in doc_scores.items():\n",
        "                f.write(f\"{query_id}\\t{doc_id}\\t{score}\\n\")\n",
        "                \n",
        "    return calculate_metrics(qrels, run)\n",
        "\n",
        "\n",
        "def calculate_metrics(qrels, run):\n",
        "    metrics = [AP, nDCG, nDCG@10, Precision@10, Recall@10, RR]\n",
        "    return {str(metric): calc_aggregate([metric], qrels, run) for metric in metrics}\n",
        "\n",
        "\n",
        "def compare_retrievals(queries, qrels, k=10, corpus=None):\n",
        "    print(\"Running embedding-based retrieval...\")\n",
        "    embedding_metrics = evaluate_embedding_retrieval(queries, qrels, k)\n",
        "    \n",
        "    print(\"\\nRunning BM25 retrieval...\")\n",
        "    bm25_metrics = evaluate_bm25_retrieval(queries, qrels, k, corpus)\n",
        "    \n",
        "    print(\"\\nComparison Results:\")\n",
        "    print(f\"{'Metric':25} {'Embedding':12} {'BM25':12}\")\n",
        "    print(\"-\" * 44)\n",
        "    \n",
        "    for metric, emb_score in embedding_metrics.items():\n",
        "        bm25_score = bm25_metrics[metric]\n",
        "        if isinstance(emb_score, dict):\n",
        "            for k, v in emb_score.items():\n",
        "                metric_name = f\"{metric}\"\n",
        "                print(f\"{metric_name:25} {v:.3f}       {bm25_score[k]:.3f}\")\n",
        "        else:\n",
        "            print(f\"{str(metric):25} {emb_score:.3f}       {bm25_score:.3f}\")\n",
        "\n",
        "\n",
        "# Usage\n",
        "qrels = load_qrels(\"evaluation/qrels.tsv\")\n",
        "print(f\"Loaded {len(qrels)} QRELS and {len(queries)} questions.\\n\")\n",
        "compare_retrievals(queries, qrels, k=5, corpus=corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TOdo: add some considerations about the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bxLzRQCaimn"
      },
      "source": [
        "## Loading the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld0Lry9LTLPJ"
      },
      "source": [
        "After conducting some research, we chose to work with one of Meta's latest smaller open-source Llama models available at the time of writing this notebook: [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), released the *25th of September 2024*. We specifically selected the *instruct* variant because it is pre-trained to follow basic instructions, offering a more user-friendly and fine-tuned experience compared to the standard non-instruct version (which is also available). Below, we review this model's [specifications](https://llamaimodel.com/requirements-3-2/):\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Llama 3.2 3B Instruct Model Specifications**                    | **Requirement**       | **Details**                                                                                  |\n",
        "|----------------------------------|-----------------------|----------------------------------------------------------------------------------------------|\n",
        "| Parameters                       | 3 billion             |                                                                                              |\n",
        "| Context Length                   | 128,000 tokens        |                                                                                              |\n",
        "| **Hardware Requirements**        |                       |                                                                                              |\n",
        "| CPU and RAM                      |                       | CPU: Multicore processor <br> RAM: Minimum of 16 GB recommended                              |\n",
        "| GPU                              |                       | NVIDIA RTX series (for optimal performance), at least 8 GB VRAM                              |\n",
        "| **Estimated GPU Memory Requirements** |                       |                                                                                              |\n",
        "| Higher Precision Modes           | BF16/FP16             | ~6.5 GB                                                                                      |\n",
        "| Lower Precision Modes            | FP8                   | ~3.2 GB                                                                                      |\n",
        "|                                  | INT4                  | ~1.75 GB                                                                                     |\n",
        "| **Software Requirements**        |                       |                                                                                              |\n",
        "| Software Dependencies            |                       | Frameworks: PyTorch <br> Libraries: Hugging Face Transformers (version 4.45.0 or higher), CUDA |\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVuakpG-TLPJ"
      },
      "outputs": [],
      "source": [
        "print(\"Is bf16 supported: \",torch.cuda.is_bf16_supported())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hw22_isTLPK"
      },
      "source": [
        "The `bitsandbytes` library is a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n",
        "\n",
        "The library includes quantization primitives for 8-bit & 4-bit operations, through `bitsandbytes.nn.Linear8bitLt` and `bitsandbytes.nn.Linear4bit` and 8-bit optimizers through bitsandbytes.optim module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYAVifb8TLPK"
      },
      "source": [
        "#### Quantization Choices\n",
        "\n",
        "We chose a *middle ground* by loading the model with **4-bit quantization** while maintaining **BFloat16** precision for computation.\n",
        "\n",
        "The use of 4-bit quantization reduces the precision of the model’s weights to just 4 bits per value, significantly lowering memory usage and accelerating inference. This method retains only the most essential information, sacrificing some numerical precision, but allows for larger models to be handled on GPUs with limited memory. Despite the weights being quantized to 4 bits, the model still performs computations in **16-bit floating point (BFloat16)** precision. BFloat16 is a 16-bit format that preserves much of the dynamic range of floating-point operations while requiring less memory than the traditional 32-bit format. This way, while the 4-bit quantization reduces the memory footprint of the model weights, the computation is performed in BFloat16, optimizing performance on modern GPUs that are tailored for BFloat16 operations. This configuration strikes a balance between computational efficiency and numerical precision, enabling fast inferences with minimal memory usage without significant loss in result quality.\n",
        "\n",
        "Given the hardware constraints, we opted for **4-bit quantization** using `BitsAndBytes` (as explained later). This approach greatly reduces the memory footprint and speeds up inference, ensuring acceptable performance for our use case. Without this configuration, the GPU’s memory usage was consistently at 100%, and inference times were approximately 2-3 minutes per query. With 4-bit quantization, memory usage drops to about 6GB, preventing GPU overload and reducing inference time to around 30 seconds per query, delivering satisfactory results. Further details on the quantization process will be provided later.\n",
        "\n",
        "However, we also need to consider that both the embedding model and the LLM must be loaded into memory, which adds another layer of complexity to the memory management. This requires careful balancing, as the total memory usage must accommodate both the LLM and the embedding model simultaneously. We will address how we manage this in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnpvz_Wxaimn"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Quantization is a technique that reduces the precision of the model’s weights to make it run faster and consume less memory, often at the cost of a slight reduction in model accuracy or quality\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                      # Lower precision reduces memory usage and can speed up inference (maybe try 8)\n",
        "    bnb_4bit_use_double_quant=True,         # Using double quantization can help reduce the loss in accuracy associated with quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",              # Normal Float 4-bit quantization, a scheme that may preserve model quality better than straightforward quantization methods\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # The internal compute dtype used during inference. bfloat16 (BF16) is often chosen because it’s efficient on modern accelerators\n",
        "    llm_int8_enable_fp32_cpu_offload=True   # Enable FP32 CPU offload\n",
        ")\n",
        "\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map={\"\": device},  # ensure all modules are on GPU\n",
        "    quantization_config=bnb_config,\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = llm_model.generation_config\n",
        "generation_config.max_new_tokens = 500                    # the maximum number of new tokens the model will generate (long outputs might be more prone to off-topic or repetitive content)\n",
        "generation_config.min_new_tokens = 10                     # the minimum number of new tokens the model will generate\n",
        "generation_config.temperature = 0.7                       # it controls the randomness of the generation, lower temp means more deterministic, conservative (less creative) and repetitive answers [about 0.1-1.2]\n",
        "generation_config.top_p = 0.7                             # nucleus sampling controls how the model picks words based on their cumulative probability, lower value (0.5) means safer, more coherent text but less diverse [about 0.5-0.9]\n",
        "generation_config.num_return_sequences = 1                # how many separate output sequences are returned for each generation prompt, get multiple different answers in one go, useful for picking the best response from several tries\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id   # special token IDs that represent padding and the end-of-sequence token. Generally, these are set to ensure the model knows when to stop and how to handle inputs of different lengths\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.repetition_penalty = 1.4                # discourages the model from repeating the same phrases or tokens over and over [about 1.0-2.0] (high value cause the model to avoid some tokens even if they are contextually appropriate)\n",
        "generation_config.num_beams = 5                           # the number of beams used in beam search, higher value means more diverse answers but also slower generation\n",
        "generation_config.early_stopping = True                   # whether to stop the beam search when at least num_beams sentences are finished per batch or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"what is web crawling?\"\n",
        "tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "print(len(tokenizer.encode(text, add_special_tokens=False)))\n",
        "\n",
        "decoded_text = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1vCxFFxWNSM"
      },
      "source": [
        "We loaded various things on the GPU up to know. Still, with the proposed configuration for all the pipeline components, GPU usage should be acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNwOi2dHTLPK"
      },
      "outputs": [],
      "source": [
        "# Check GPU usage\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # in GB\n",
        "memory_allocated_gb = torch.cuda.memory_allocated(device) / (1024 ** 3)  # in GB\n",
        "print(f\"Memory Allocated: {memory_allocated_gb:.2f} GB\")\n",
        "print(f\"Total memory usage: {(memory_allocated_gb / gpu_memory_gb) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTIj3ob0aimn"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_documents(query, k=5):\n",
        "\n",
        "    query_embedding = embeddings_model.encode(\n",
        "            query,\n",
        "            prompt_name=\"query\",\n",
        "            dataset_embeddings=dataset_embeddings,  # this is the contexualized embeddings of the minicorpus\n",
        "            convert_to_tensor=True\n",
        "        )\n",
        "         \n",
        "    similarities = embeddings_model.similarity(query_embedding, doc_embeddings)\n",
        "    topk_values, topk_indices = similarities.topk(k)\n",
        "    topk_indices = topk_indices.cpu().numpy()[0]\n",
        "    topk_values = topk_values.cpu().numpy()[0]\n",
        "    \n",
        "    return [corpus[idx] for idx in topk_indices], topk_values\n",
        "\n",
        "# test the function\n",
        "query = \"How many bytes can UTF-8 use to encode a character?\"\n",
        "print(f\"Query: {query}\")\n",
        "documents, distances = retrieve_relevant_documents(query)\n",
        "for doc, distance in (zip(documents, distances)):\n",
        "    print(f\"(score: {distance:.4f}) {doc[:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dZoEEO3caimo"
      },
      "outputs": [],
      "source": [
        "base_prompt = '''You are an AI assistant expert of Information Retrieval.\n",
        "Your task is to provide answers to user questions based on the provided context.\n",
        "\n",
        "Instructions:\n",
        "- Use the provided context to construct your answers.\n",
        "- Avoid directly quoting examples or specific details from the context unless they are explicitly required to answer the question.\n",
        "- Paraphrase any necessary details from the context in a way that does not depend on the user's knowledge of the full context.\n",
        "- The context may contain more information than needed to answer the question. Use only the information that is relevant to the question.\n",
        "- If the context lacks sufficient information, provide the best possible answer using general knowledge or state: \"The provided context does not have the answer.\"\n",
        "\n",
        "Your goal is to ensure that your response is complete and clear even if the user has no access to the context.\n",
        "\n",
        "User question: {user_query}\n",
        "\n",
        "Provided Context:\n",
        "{chunks_information}\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdnGxkarWNSM"
      },
      "source": [
        "## Perform Queries to the RAG Pipeline\n",
        "\n",
        "Now, we're ready to test the pipeline. The following function executes a query by performing the steps outlined below:\n",
        "\n",
        "1. **Retrieve Relevant Documents**: The function invokes `retrieve_relevant_documents` (defined in the coresponding section) to fetch the top 5 relevant documents to the query, along with their similarity scores\n",
        "2. **Filter Documents**: Documents with a similarity score below 0.5 (calculated as `score`) are discarded, retaining only those with sufficient relevance. If no documents meet the relevance threshold, a message indicating that no relevant documents were found is returned.\n",
        "3. **Print the results**: If `print_retrieved_documents` is set to `True`, retrieved relevant documetns are also printed on the terminal.\n",
        "4. **Prepare the Prompt**: The filtered documents are used to create a prompt, which is passed to the model to generate a response. The prompt includes the user's query and the relevant document chunks.\n",
        "5. **Model Inference**: The prompt is tokenized and passed to the RAG model. The model generates a response using beam search (with 5 beams) and applies early stopping.\n",
        "6. **Generate and Return Response**: The function decodes the generated tokens and returns the resulting sequence as the response.\n",
        "\n",
        "This process ensures that the generated response is grounded in relevant documents, providing more accurate, context-aware answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9ndL7blvWNSN"
      },
      "outputs": [],
      "source": [
        "def query_rag_model(query, base_prompt=base_prompt, print_retrieved_documents=True):\n",
        "    \"\"\"\n",
        "    Passes the user query to the RAG model and returns the generated answer.\n",
        "    \"\"\"\n",
        "\n",
        "    documents, scores = retrieve_relevant_documents(query, k=5)\n",
        "\n",
        "    if print_retrieved_documents:\n",
        "        print(\"Filtered documents and their scores: \")\n",
        "        for doc, score in zip(documents, scores):\n",
        "            print(f\"Score: {1-score:.4f}, {doc}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # Filters out documents with a similarity score below 0.5\n",
        "    filtered_documents = [doc for doc, score in zip(documents, scores) if score >= 0.5]\n",
        "\n",
        "    # If no relevant documents are found, return a message\n",
        "    if not filtered_documents:\n",
        "        print(f\"No relevant documents found for query: {query}\")\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    # Prepare the prompt for the model\n",
        "    prompt = base_prompt.format(user_query=query, chunks_information=\"\\n\".join(filtered_documents))\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # print(\"PROMPT:\", prompt)\n",
        "\n",
        "    with torch.inference_mode(): # disables gradient computation during model execution\n",
        "        outputs = llm_model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "    generated_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    return generated_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWcv6y7WNSN"
      },
      "source": [
        "Try it out by changing the query! Try also to change `print_retrieved_documents` to `True` to see what documents are influencing the produced output.\n",
        ">This operation takes ~15/30 seconds on our machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPqDR2R0WNSN"
      },
      "outputs": [],
      "source": [
        "response = query_rag_model(\"what is information retrieval?\", base_prompt=base_prompt, print_retrieved_documents=False)\n",
        "print(\"Response: \", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU5nIjpEWNSN"
      },
      "source": [
        "# Evaluating model responses\n",
        "\n",
        "The following section covers experiments based on how the model replies to **open-answer** questions.\n",
        "\n",
        "Questions were stored in this format, and were manually tailored by us in order to asses the capabilities of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Tfvfg0WNSN"
      },
      "source": [
        "The file,  `open_questions.json` file contains open-answer questions, where an entry looks like:\n",
        "\n",
        "```json\n",
        "{\n",
        "        \"question_id\": \"1\",\n",
        "        \"question\": \"What is Jaccard Coefficient?\"\n",
        "}, ...\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Field**       | **Content**                                     |\n",
        "|------------------|-------------------------------------------------|\n",
        "| `question_id`    | Unique identifier of the question        |\n",
        "| `question`       | The question text                              |\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5W-q3VYEWNSN"
      },
      "outputs": [],
      "source": [
        "evaluation_folder_path = \"./evaluation/\"\n",
        "questions_file_path = evaluation_folder_path + \"open_questions.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2U0YrdmWNSN"
      },
      "outputs": [],
      "source": [
        "open_questions = extract_questions(questions_file_path)\n",
        "\n",
        "print(f\"Loaded {len(open_questions)} open questions.\")\n",
        "print(open_questions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMurWVAJWNSO"
      },
      "source": [
        "### Evaluating the model on open-questions\n",
        "\n",
        "`process_open_questions(queries, folder_path)` is a function used to run experiments over a list of open-answer questions, taking as input the list of questions with `queries` and folder path `folder_path` for results storing. It does the following:\n",
        "\n",
        "1. Delete previous results if already done, create `res` folder otherwise\n",
        "2. Process one-by-one queries via `response = query_rag_model(query, base_prompt)`\n",
        "\n",
        "Note that:\n",
        "* No results are returned since are already store on `.txt` files\n",
        "* retrieve_relevant_documents is called only to write on .txt files documents retrieved, but it is also called into `query_rag_model` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "AcVeNt8AWNSO"
      },
      "outputs": [],
      "source": [
        "def process_open_questions(queries, folder_path, model=\"RAG\"):\n",
        "    \"\"\"\n",
        "    Processes a list of open questions, retrieves relevant documents,\n",
        "    and saves the responses to text files in a specified folder.\n",
        "\n",
        "    Parameters:\n",
        "    - queries (list): List of queries to process.\n",
        "    - folder_path (str): Path to the folder where results will be saved.\n",
        "    - model (str): Name of the model to use for processing the queries.\n",
        "    \"\"\"\n",
        "    results_folder = os.path.join(folder_path, model+\"-open-questions\")\n",
        "    if os.path.exists(results_folder):\n",
        "        # Clear the folder if it exists\n",
        "        for file_name in os.listdir(results_folder):\n",
        "            file_path = os.path.join(results_folder, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "    else:\n",
        "        # Create the folder if it doesn't exist\n",
        "        os.makedirs(results_folder)\n",
        "\n",
        "    # Process each query and save the response\n",
        "    for idx, query in enumerate(tqdm(queries, desc=\"Processing queries\"), start=1):\n",
        "            if model ==\"RAG\":\n",
        "                # Get the model's response\n",
        "                response = query_rag_model(query['question'], base_prompt, print_retrieved_documents=False)\n",
        "            elif model == \"LLM\":\n",
        "                prompt = \"You are an AI assistant expert of Information Retrieval.\\n\"+ query['question']\n",
        "                encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "                with torch.inference_mode(): # disables gradient computation during model execution\n",
        "                    outputs = llm_model.generate(\n",
        "                        input_ids=encoding.input_ids,\n",
        "                        attention_mask=encoding.attention_mask,\n",
        "                        generation_config=generation_config,\n",
        "                    )\n",
        "\n",
        "                generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "                response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "            else:\n",
        "                raise ValueError(\"Invalid model name. Use either 'RAG' or 'LLM'.\")\n",
        "\n",
        "            # Construct the file path for the current query\n",
        "            file_path = os.path.join(results_folder, f\"open_question_{idx}.mmd\")\n",
        "\n",
        "            # Save the query, response, and documents to a text file\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(f\"Query {idx}: {query['question']}\\n\\n\")\n",
        "                file.write(f\"Reply {idx}: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purpouse of running the whole notebook, we set `evaluation = False` to avoid each time the computation of the whole `open_questions` set. Set it to `True` to re-compute them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NB6_0hr_WNSO"
      },
      "outputs": [],
      "source": [
        "if evaluation:\n",
        "    process_open_questions(open_questions, evaluation_folder_path, model=\"RAG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "oJiQS1_rWNSO"
      },
      "outputs": [],
      "source": [
        "if evaluation:\n",
        "    process_open_questions(open_questions, evaluation_folder_path, model=\"LLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fryWDCryZe00"
      },
      "source": [
        "### Testing Procedure\n",
        "As reported above, to evaluate the performance of the RAG pipeline we conducted a manual assessment using a set of **60 open-ended questions** related to information retrieval. The evaluation consisted of two phases:\n",
        "1. RAG Model Testing:\n",
        "   - The RAG pipeline was used to answer the questions, incorporating both retrieved documents and the language model's generation capabilities.\n",
        "   - The pipeline was designed to act as an \"AI assistant\" and \"information retrieval expert,\" as explicitly stated in the prompt.\n",
        "2. Base Model Testing:\n",
        "   - The same 60 questions were tested using the base language model without the RAG pipeline.\n",
        "   - To ensure a fair comparison, efforts were made to align the prompts of the two models. For the base model, the adjusted prompt explicitly mentioned the \"information retrieval\" context to avoid bias in favor of the RAG pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### Results\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Evaluation Metric**    | **Base Model** | **RAG Pipeline** |\n",
        "|-|-|-|\n",
        "| Total Questions Evaluated | 60| 60|\n",
        "| Correct Answers           | 23| 57|\n",
        "|*Accuracy (%)*              |**38.3%**| **95%**|\n",
        "\n",
        "</div>\n",
        "\n",
        "<div></div>\n",
        "\n",
        "---\n",
        "\n",
        "#### Analysis of Results:\n",
        "1. The RAG pipeline demonstrated a 95% accuracy rate, significantly outperforming the base model, which achieved only 38.3% accuracy.\n",
        "2. This substantial improvement highlights the value of integrating a retrieval mechanism into the pipeline. By grounding the model's responses in relevant documents, the RAG pipeline could produce more accurate and contextually aligned answers.\n",
        "\n",
        "3. For the base model, the incorrect answers were due to:\n",
        "     - Hallucinations: The model generated plausible-sounding but incorrect information.\n",
        "     - Lack of Specificity: Responses were vague or unrelated to the query.\n",
        "4. In contrast, the RAG pipeline failed in only 3 cases where the answers were indeed correct, but they didn't come from the given context but from general knowledge, so we labled them as incorrect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ1KmZNwWNSO"
      },
      "source": [
        "# Gradio Interface <div align=\"center\"><img src=\"https://www.gradio.app/_app/immutable/assets/gradio.CHB5adID.svg\" alt=\"Gradio Logo\" width=\"200\"></div>\n",
        "\n",
        "Lastly, we provide a **Gradio** interface to make interacting with the RAG pipeline both user-friendly and accessible. [Gradio](https://gradio.app) is a Python library that enables developers to quickly create customizable, interactive web-based interfaces. It is widely appreciated for its simplicity and flexibility.\n",
        "\n",
        "The interface is launched in the next cell through the final command `demo.launch(debug=True)`\n",
        "\n",
        "> This will also start a local server and generate a link (usually `http://127.0.0.1:7860`) that you can open in your browser. On Colab this will be different, but the you can find the url in the console output\n",
        "\n",
        "The interface is straightforward:\n",
        "- You can type your questions or prompts for the RAG pipeline into the **input field**.\n",
        "- The interface estimated inference waiting time (based on the last response waiting time), then the generated answer.\n",
        "\n",
        "If the interface is launched with the `debug=True` option (as in this case), it will also print the retrieved documents and their scores in the console; this can be useful for gaining insights into the system's behavior. Otherwise, the console prints are suppressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyeIkuECWNSO"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "latex_delimiters = [\n",
        "    {\"left\": \"\\\\[\", \"right\": \"\\\\]\", \"display\": True},  # Formulas in display mode\n",
        "    {\"left\": \"\\\\(\", \"right\": \"\\\\)\", \"display\": False},  # Formulas in inline mode\n",
        "]\n",
        "\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    .input-box { border: 1px solid #ccc; border-radius: 4px; padding: 10px; margin: 10px 0; }\n",
        "    .output-box { border: 1px solid #ccc; border-radius: 4px; padding: 10px; margin: 10px 0; background-color: #373535; height: auto; }\n",
        "\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"# RAG Model Query Interface\")\n",
        "    gr.Markdown(f\"Ask questions to the RAG model and get answers based on the provided PDF context (*{PDF_NAME}*).\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_text = gr.Textbox(\n",
        "                label=\"Enter your query\",\n",
        "                placeholder=\"Type your question here...\",\n",
        "                elem_classes=[\"input-box\"]\n",
        "            )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_text = gr.Markdown(\n",
        "                label=\"Answer\",\n",
        "                elem_classes=[\"output-box\"],\n",
        "                latex_delimiters=latex_delimiters\n",
        "                )\n",
        "\n",
        "\n",
        "    input_text.submit(\n",
        "        fn=query_rag_model,\n",
        "        inputs=input_text,\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "# launch the interface\n",
        "demo.launch(debug=True, show_error=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im415tVOWNSP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "lm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
