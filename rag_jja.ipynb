{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYv4oHYlxt7w",
        "outputId": "e7f1865f-8ad9-4255-d61d-0c54614093d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface-hub python-dotenv transformers sentence-transformers langchain-community langchain-huggingface langchain tqdm regex gradio unidecode pymupdf chromadb bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DwGS9pBI4MyR",
        "outputId": "6cd35d8a-077b-4532-8850-6b4f162d0506"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.29.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "#!pip install -U bitsandbytes\n",
        "\n",
        "!pip install -q huggingface-hub python-dotenv transformers sentence-transformers langchain-community langchain-huggingface langchain tqdm regex gradio unidecode pymupdf chromadb bitsandbytes\n",
        "!pip install -q protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6hYBdAznF95f"
      },
      "outputs": [],
      "source": [
        "# non serve\n",
        "#!wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json\n",
        "#!wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_fullwiki_v1.json\n",
        "#!wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "_VKy-c1YL_Wx",
        "outputId": "240a7bfb-9a45-43c5-9d2e-3fdf24680191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'import requests\\nimport zipfile\\nimport os\\n\\n# URL del file ZIP\\nurl = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/hotpotqa.zip\"\\n\\n# Nome del file ZIP locale\\nzip_file_name = \"hotpotqa.zip\"\\n\\n# Cartella di estrazione\\nextract_dir = \"hotpotqa_dataset\"\\n\\n# Scaricare il file ZIP\\nprint(\"Scaricamento del file ZIP...\")\\nresponse = requests.get(url, stream=True)\\nif response.status_code == 200:\\n    with open(zip_file_name, \"wb\") as f:\\n        for chunk in response.iter_content(chunk_size=1024):\\n            f.write(chunk)\\n    print(f\"Scaricamento completato: {zip_file_name}\")\\nelse:\\n    print(f\"Errore durante il download: {response.status_code}\")\\n    exit()\\n\\n# Estrarre il contenuto del file ZIP\\nprint(f\"Estrazione del file ZIP in {extract_dir}...\")\\nos.makedirs(extract_dir, exist_ok=True)\\nwith zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\\n    zip_ref.extractall(extract_dir)\\nprint(f\"Estrazione completata. Contenuti disponibili nella cartella: {extract_dir}\")\\n\\n# Facoltativo: Rimuovere il file ZIP dopo l\\'estrazione\\n#os.remove(zip_file_name)\\n#print(f\"File ZIP rimosso: {zip_file_name}\")'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# questo serve\n",
        "'''import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# URL del file ZIP\n",
        "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/hotpotqa.zip\"\n",
        "\n",
        "# Nome del file ZIP locale\n",
        "zip_file_name = \"hotpotqa.zip\"\n",
        "\n",
        "# Cartella di estrazione\n",
        "extract_dir = \"hotpotqa_dataset\"\n",
        "\n",
        "# Scaricare il file ZIP\n",
        "print(\"Scaricamento del file ZIP...\")\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(zip_file_name, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            f.write(chunk)\n",
        "    print(f\"Scaricamento completato: {zip_file_name}\")\n",
        "else:\n",
        "    print(f\"Errore durante il download: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Estrarre il contenuto del file ZIP\n",
        "print(f\"Estrazione del file ZIP in {extract_dir}...\")\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(f\"Estrazione completata. Contenuti disponibili nella cartella: {extract_dir}\")\n",
        "\n",
        "# Facoltativo: Rimuovere il file ZIP dopo l'estrazione\n",
        "#os.remove(zip_file_name)\n",
        "#print(f\"File ZIP rimosso: {zip_file_name}\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NSxFlH2SG5e",
        "outputId": "b018be3d-4864-4e4d-9109-502d18026303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': '12', 'title': 'Anarchism', 'text': 'Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary and harmful.', 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=12'}}\n",
            "{'_id': '25', 'title': 'Autism', 'text': \"Autism is a neurodevelopmental disorder characterized by impaired social interaction, impaired verbal and non-verbal communication, and restricted and repetitive behavior. Parents usually notice signs in the first two years of their child's life. These signs often develop gradually, though some children with autism reach their developmental milestones at a normal pace and then regress. The diagnostic criteria require that symptoms become apparent in early childhood, typically before age three.\", 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=25'}}\n",
            "{'_id': '39', 'title': 'Albedo', 'text': 'Albedo ( ) is a measure for reflectance or optical brightness (Latin \"albedo,\" \"whiteness\") of a surface. It is dimensionless and measured on a scale from zero (corresponding to a black body that absorbs all incident radiation) to one (corresponding to a white body that reflects all incident radiation).', 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=39'}}\n",
            "Numero totale di documenti: 5233329\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def print_first_three_elements(file_path):\n",
        "    \"\"\"\n",
        "    Legge un file JSON Lines (.jsonl) e stampa i primi tre elementi.\n",
        "\n",
        "    :param file_path: Percorso al file corpus.jsonl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            count = 0\n",
        "            for line in file:\n",
        "                # Decodifica ogni riga come oggetto JSON\n",
        "                data = json.loads(line.strip())\n",
        "                print(data)\n",
        "                count += 1\n",
        "                # Ferma dopo aver letto 3 elementi\n",
        "                if count == 3:\n",
        "                    break\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File non trovato: {file_path}\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Errore nel parsing del file JSONL: {e}\")\n",
        "\n",
        "# Esempio di utilizzo\n",
        "file_path = \"hotpotqa/hotpotqa/corpus.jsonl\"\n",
        "print_first_three_elements(file_path)\n",
        "# Calcola e stampa il numero totale di documenti\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    total_documents = sum(1 for _ in file)\n",
        "print(f\"Numero totale di documenti: {total_documents}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not running in Google Colab. Using load_dotenv to get HF_TOKEN.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Running in Google Colab. Using userdata to get HF_TOKEN.\")\n",
        "except ModuleNotFoundError:\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "    print(\"Not running in Google Colab. Using load_dotenv to get HF_TOKEN.\")\n",
        "\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available. Working on GPU.\n",
            "GPU name: NVIDIA GeForce RTX 3070 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    print(\"MPS is available. Working on MPS.\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    print(\"CUDA is available. Working on GPU.\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA and MPS not available. Working on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of documents: 267.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing documents: 100%|██████████| 5233329/5233329 [01:41<00:00, 51432.87it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# Carica documenti dal file JSON Lines\n",
        "def load_documents(file_path):\n",
        "    documents = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line.strip())\n",
        "            if \"text\" in data:\n",
        "                documents.append(Document(page_content=data[\"text\"], metadata={\"title\": data.get(\"title\", \"\")}))\n",
        "    total_length = sum(len(doc.page_content) for doc in documents)\n",
        "    avg_length = total_length / len(documents) if documents else 0\n",
        "    print(f\"Average length of documents: {avg_length:.3f}\")\n",
        "    return documents\n",
        "\n",
        "# Preprocessa i testi\n",
        "def preprocess_text(text):\n",
        "    # Rimuovi formule matematiche\n",
        "    text = re.sub(r'\\$.*?\\$', '', text)\n",
        "    # Rimuovi URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Rimuovi tag HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Rimuovi caratteri non ASCII\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # Rimuovi caratteri speciali e numeri, mantenendo solo lettere e punteggiatura di base\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,;:!?\\'\"-]', '', text)\n",
        "    # Normalizza Unicode\n",
        "    text = unidecode(text)\n",
        "    # Rimuovi spazi multipli\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Percorso al file del dataset\n",
        "documents = load_documents(file_path)\n",
        "\n",
        "# Preprocessa i contenuti\n",
        "for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
        "    doc.page_content = preprocess_text(doc.page_content)\n",
        "\n",
        "# TODO : i documenti sono già corti, non serve splittarli\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# split_documents = []\n",
        "# for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
        "#     if len(doc[\"page_content\"]) > 1500:\n",
        "#         chunks = text_splitter.split_text(doc[\"page_content\"])\n",
        "#         split_documents.extend([{\"page_content\": chunk, \"metadata\": doc[\"metadata\"]} for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53f473b78be24bb3a5e7db428eca8fd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a334930b80941609a35ef1b307c7886",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5d9ed8b1c6c4d89999da7048da00979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b18fd26c29445a6b0e1a83ac5487336",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b03ce8c987842caaee439c36f80c2e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9251d0ec8014e18ba75e3789118e2a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating embeddings:   0%|          | 150/5233329 [00:01<13:48:31, 105.27document/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "507e6882fc0b4a09afa0fdbf9bb05f59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating embeddings:   1%|          | 31660/5233329 [05:21<14:43:45, 98.10document/s] "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Configurazione del modello\n",
        "model_kwargs = {'device': device}\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs=model_kwargs)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"facebook/contriever\", model_kwargs=model_kwargs)\n",
        "\n",
        "\n",
        "# Calcolo delle embedding con tqdm\n",
        "embedding_results = []\n",
        "for doc in tqdm(documents, desc=\"Calculating embeddings\", unit=\"document\"):\n",
        "    embedding = embeddings.embed_query(doc.page_content)\n",
        "    embedding_results.append({\"embedding\": embedding, \"metadata\": doc.metadata, \"content\": doc.page_content})\n",
        "\n",
        "# Preparazione dei documenti per Chroma\n",
        "chroma_docs = [{\"page_content\": item[\"content\"], \"metadata\": item[\"metadata\"]} for item in embedding_results]\n",
        "\n",
        "# Creazione del database Chroma\n",
        "print(\"Creating Chroma database...\")\n",
        "db = Chroma.from_documents(chroma_docs, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "# Creazione del retriever\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.repetition_penalty = 1.6  # Add repetition penalty\n",
        "\n",
        "\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8TcYdrhxKxb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Initialize the sentence-transformers model\n",
        "embedder_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder_model = SentenceTransformer(embedder_model_name)\n",
        "\n",
        "def embedder(chunk):\n",
        "    embeddings = embedder_model.encode(chunk, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()\n",
        "\n",
        "\n",
        "# Use the retriever to get the top_k_chunks for a given query\n",
        "def search(query, retriever, k=10):\n",
        "    results = retriever.invoke(query, k=k)\n",
        "    return [result.page_content for result in results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PROMPT AND ANSWER QUESTION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"You are an AI assistant for RAG. Your task is to understand the user question, and provide an answer using the provided contexts.\n",
        "\n",
        "Your answers are correct, high-quality, and written by a domain expert. If the provided context does not contain the answer, simply state, \"The provided context does not have the answer.\"\n",
        "\n",
        "User question: {user_query}\n",
        "\n",
        "Contexts:\n",
        "{chunks_information}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def answer_questions(questions):\n",
        "    count = 0\n",
        "    error = 0\n",
        "    results = {}\n",
        "\n",
        "    current_time = time.strftime(\"%m%d-%H%M%S\")\n",
        "\n",
        "    pbar = tqdm(questions, total=len(questions), desc=\"Answering questions...\", unit=\"question\")\n",
        "    for q in pbar:\n",
        "        top_k_chunks = search(q['question'], retriever, k=10)\n",
        "        retrieved_chunks = [chunk for chunk in top_k_chunks]\n",
        "        prompt = base_prompt.format(user_query=q['question'], chunks_information=\"\\n\".join(retrieved_chunks))\n",
        "        encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(\n",
        "                input_ids=encoding.input_ids,\n",
        "                attention_mask=encoding.attention_mask,\n",
        "                generation_config=generation_config,\n",
        "                num_beams=5,  # Use beam search for better results\n",
        "                early_stopping=True,  # Stop early if all beams finish\n",
        "            )\n",
        "\n",
        "        # Exclude the prompt tokens from the generated output\n",
        "        generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "        generated_unpreprocessed_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "        match = re.search(r'\\b[1-4]\\b', generated_unpreprocessed_sequence)\n",
        "        answer = match.group(0) if match else \"\"  # first number found or empty string\n",
        "\n",
        "        with open(f\"quiz/runs_basemodel/quiz_answers_{current_time}.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Question: {q['question']}\\nAnswer: {answer}\\nCorrect answer:{q['correct']}\\nGenerated unpreprocessed sequence: {generated_unpreprocessed_sequence}\\n--------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "        results[q['question_id']] = answer\n",
        "\n",
        "        if len(answer) != 1 or answer not in \"1234\":\n",
        "            error += 1\n",
        "        else:  # the format is correct, now check if the answer is correct\n",
        "            if str(q['correct']) == answer:\n",
        "                count += 1\n",
        "        pbar.set_postfix(Corrects=f\"{count}/{len(questions)}\", Errors=error)\n",
        "\n",
        "    print(\"-------------------------\\tFINISHED RUN. Error count: \", error, \"-------------------------\")\n",
        "    return results, count / len(questions) * 100\n",
        "\n",
        "# Example questions\n",
        "# questions = [\n",
        "#     {\"question\": \"\", \"correct\": \"1\", \"question_id\": \"1\"},\n",
        "#     # Add more questions as needed\n",
        "# ]\n",
        "\n",
        "# results, score = answer_questions(questions)\n",
        "# print(f\"Final score: {score}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRADIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def query_rag_model(user_query):\n",
        "    top_k_chunks = search(user_query, retriever, k=10)\n",
        "    retrieved_chunks = [chunk for chunk in top_k_chunks]\n",
        "    prompt = base_prompt.format(user_query=user_query, chunks_information=\"\\n\".join(retrieved_chunks))\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "    generated_unpreprocessed_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    return generated_unpreprocessed_sequence\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query_rag_model,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG Model Query Interface\",\n",
        "    description=\"Ask questions to the RAG model and get answers based on the provided PDF context.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
