{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliocapecchi/LM-project/blob/main/rag_jja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xYv4oHYlxt7w"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface-hub python-dotenv transformers sentence-transformers langchain-community langchain-huggingface langchain tqdm regex gradio unidecode pymupdf chromadb bitsandbytes langchain_chroma --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DwGS9pBI4MyR"
      },
      "outputs": [],
      "source": [
        "#!pip install -U bitsandbytes --progress-bar off\n",
        "!pip install -q huggingface-hub python-dotenv transformers sentence-transformers langchain-community langchain-huggingface langchain tqdm regex gradio unidecode pymupdf chromadb bitsandbytes --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VKy-c1YL_Wx",
        "outputId": "b6d08ac7-c380-42b8-92f5-916c35a20d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaricamento del file ZIP...\n",
            "Scaricamento completato: hotpotqa.zip\n",
            "Estrazione del file ZIP in hotpotqa...\n",
            "Estrazione completata. Contenuti disponibili nella cartella: hotpotqa\n"
          ]
        }
      ],
      "source": [
        "# questo serve\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# URL del file ZIP\n",
        "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/hotpotqa.zip\"\n",
        "\n",
        "# Nome del file ZIP locale\n",
        "zip_file_name = \"hotpotqa.zip\"\n",
        "\n",
        "# Cartella di estrazione\n",
        "extract_dir = \"hotpotqa\"\n",
        "\n",
        "# Scaricare il file ZIP\n",
        "print(\"Scaricamento del file ZIP...\")\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(zip_file_name, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            f.write(chunk)\n",
        "    print(f\"Scaricamento completato: {zip_file_name}\")\n",
        "else:\n",
        "    print(f\"Errore durante il download: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Estrarre il contenuto del file ZIP\n",
        "print(f\"Estrazione del file ZIP in {extract_dir}...\")\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(f\"Estrazione completata. Contenuti disponibili nella cartella: {extract_dir}\")\n",
        "\n",
        "# remove zip file after extraction\n",
        "os.remove(zip_file_name)\n",
        "print(f\"File ZIP rimosso: {zip_file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NSxFlH2SG5e",
        "outputId": "eca19a86-6468-42cd-b1ff-d90f86351de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_id': '12', 'title': 'Anarchism', 'text': 'Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary and harmful.', 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=12'}}\n",
            "{'_id': '25', 'title': 'Autism', 'text': \"Autism is a neurodevelopmental disorder characterized by impaired social interaction, impaired verbal and non-verbal communication, and restricted and repetitive behavior. Parents usually notice signs in the first two years of their child's life. These signs often develop gradually, though some children with autism reach their developmental milestones at a normal pace and then regress. The diagnostic criteria require that symptoms become apparent in early childhood, typically before age three.\", 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=25'}}\n",
            "{'_id': '39', 'title': 'Albedo', 'text': 'Albedo ( ) is a measure for reflectance or optical brightness (Latin \"albedo,\" \"whiteness\") of a surface. It is dimensionless and measured on a scale from zero (corresponding to a black body that absorbs all incident radiation) to one (corresponding to a white body that reflects all incident radiation).', 'metadata': {'url': 'https://en.wikipedia.org/wiki?curid=39'}}\n",
            "Numero totale di documenti: 5233329\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def print_first_three_elements(file_path):\n",
        "    \"\"\"\n",
        "    Legge un file JSON Lines (.jsonl) e stampa i primi tre elementi.\n",
        "\n",
        "    :param file_path: Percorso al file corpus.jsonl\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            count = 0\n",
        "            for line in file:\n",
        "                # Decodifica ogni riga come oggetto JSON\n",
        "                data = json.loads(line.strip())\n",
        "                print(data)\n",
        "                count += 1\n",
        "                # Ferma dopo aver letto 3 elementi\n",
        "                if count == 3:\n",
        "                    break\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File non trovato: {file_path}\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Errore nel parsing del file JSONL: {e}\")\n",
        "\n",
        "# Esempio di utilizzo\n",
        "file_path = \"hotpotqa/hotpotqa/corpus.jsonl\"\n",
        "print_first_three_elements(file_path)\n",
        "# Calcola e stampa il numero totale di documenti\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    total_documents = sum(1 for _ in file)\n",
        "print(f\"Numero totale di documenti: {total_documents}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfvmUDYm9yw0",
        "outputId": "e208b897-930d-45bd-e5c4-e836c8bb49b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using userdata to get HF_TOKEN.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Running in Google Colab. Using userdata to get HF_TOKEN.\")\n",
        "except ModuleNotFoundError:\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "    print(\"Not running in Google Colab. Using load_dotenv to get HF_TOKEN.\")\n",
        "\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSnS7r1K9yw0",
        "outputId": "d4a82d3c-1986-4666-86ef-bea1132a4049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Working on GPU.\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    print(\"MPS is available. Working on MPS.\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    print(\"CUDA is available. Working on GPU.\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA and MPS not available. Working on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDoKVSeH9yw0",
        "outputId": "5443707e-5eaf-43ff-fc53-f4aabfaeb939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of documents: 267.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing documents: 100%|██████████| 5233329/5233329 [03:23<00:00, 25729.75it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# Carica documenti dal file JSON Lines\n",
        "def load_documents(file_path):\n",
        "    documents = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line.strip())\n",
        "            if \"text\" in data:\n",
        "                documents.append(Document(page_content=data[\"text\"], metadata={\"title\": data.get(\"title\", \"\")}))\n",
        "    total_length = sum(len(doc.page_content) for doc in documents)\n",
        "    avg_length = total_length / len(documents) if documents else 0\n",
        "    print(f\"Average length of documents: {avg_length:.3f}\")\n",
        "    return documents\n",
        "\n",
        "# Preprocessa i testi\n",
        "def preprocess_text(text):\n",
        "    # Rimuovi formule matematiche\n",
        "    text = re.sub(r'\\$.*?\\$', '', text)\n",
        "    # Rimuovi URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Rimuovi tag HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Rimuovi caratteri non ASCII\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # Rimuovi caratteri speciali e numeri, mantenendo solo lettere e punteggiatura di base\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,;:!?\\'\"-]', '', text)\n",
        "    # Normalizza Unicode\n",
        "    text = unidecode(text)\n",
        "    # Rimuovi spazi multipli\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Percorso al file del dataset\n",
        "documents = load_documents(file_path)\n",
        "\n",
        "# Preprocessa i contenuti\n",
        "for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
        "    doc.page_content = preprocess_text(doc.page_content)\n",
        "\n",
        "# TODO : i documenti sono già corti, non serve splittarli\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# split_documents = []\n",
        "# for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
        "#     if len(doc[\"page_content\"]) > 1500:\n",
        "#         chunks = text_splitter.split_text(doc[\"page_content\"])\n",
        "#         split_documents.extend([{\"page_content\": chunk, \"metadata\": doc[\"metadata\"]} for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNZhHTWb9yw1",
        "outputId": "d05afc87-69c1-400d-fa61-835c22a1f42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-83f17a86b0a7>:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  db = Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Chroma...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing and storing batches:   0%|          | 52/40886 [00:40<11:35:36,  1.02s/it]ERROR:chromadb.db.mixins.embeddings_queue:Exception occurred invoking consumer for subscription 656eb5cafbfa46a096eaaca36c8f4cbbto topic persistent://default/default/90f30593-7186-4a52-8526-a692ea3f192f \n",
            "Processing and storing batches:   0%|          | 67/40886 [00:50<8:00:27,  1.42it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Configure the model for GPU usage\n",
        "model_kwargs = {'device': 'cuda'}  # Ensure GPU acceleration\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs=model_kwargs)\n",
        "\n",
        "# Split documents into batches for efficient processing\n",
        "batch_size = 512  # Adjust based on your GPU's memory (e.g., 128 for RTX 3070)\n",
        "\n",
        "def batch_embeddings(documents, embeddings, batch_size):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Calculating embeddings in batches\"):\n",
        "        batch = documents[i:i + batch_size]\n",
        "        batch_texts = [doc.page_content for doc in batch]\n",
        "        batch_embeddings = embeddings.embed_documents(batch_texts)  # Efficient batch processing\n",
        "        all_embeddings.extend([{\"page_content\": doc.page_content, \"metadata\": doc.metadata, \"embedding\": emb}\n",
        "                               for doc, emb in zip(batch, batch_embeddings)])\n",
        "    return all_embeddings\n",
        "\n",
        "# Calculate embeddings in batches\n",
        "embedding_results = batch_embeddings(documents, embeddings, batch_size)\n",
        "\n",
        "# Create Chroma vector store from the precomputed embeddings\n",
        "print(\"Creating Chroma database...\")\n",
        "chroma_docs = [{\"page_content\": item[\"page_content\"], \"metadata\": item[\"metadata\"]} for item in embedding_results]\n",
        "db = Chroma.from_documents(chroma_docs, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "# Create retriever\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGawGJNo9yw1"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.repetition_penalty = 1.6  # Add repetition penalty\n",
        "\n",
        "\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8TcYdrhxKxb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Initialize the sentence-transformers model\n",
        "embedder_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder_model = SentenceTransformer(embedder_model_name)\n",
        "\n",
        "def embedder(chunk):\n",
        "    embeddings = embedder_model.encode(chunk, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()\n",
        "\n",
        "\n",
        "# Use the retriever to get the top_k_chunks for a given query\n",
        "def search(query, retriever, k=10):\n",
        "    results = retriever.invoke(query, k=k)\n",
        "    return [result.page_content for result in results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxkHaXbI9yw2"
      },
      "source": [
        "## PROMPT AND ANSWER QUESTION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFG2gq239yw2"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"You are an AI assistant for RAG. Your task is to understand the user question, and provide an answer using the provided contexts.\n",
        "\n",
        "Your answers are correct, high-quality, and written by a domain expert. If the provided context does not contain the answer, simply state, \"The provided context does not have the answer.\"\n",
        "\n",
        "User question: {user_query}\n",
        "\n",
        "Contexts:\n",
        "{chunks_information}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oW899qz9yw2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def answer_questions(questions):\n",
        "    count = 0\n",
        "    error = 0\n",
        "    results = {}\n",
        "\n",
        "    current_time = time.strftime(\"%m%d-%H%M%S\")\n",
        "\n",
        "    pbar = tqdm(questions, total=len(questions), desc=\"Answering questions...\", unit=\"question\")\n",
        "    for q in pbar:\n",
        "        top_k_chunks = search(q['question'], retriever, k=10)\n",
        "        retrieved_chunks = [chunk for chunk in top_k_chunks]\n",
        "        prompt = base_prompt.format(user_query=q['question'], chunks_information=\"\\n\".join(retrieved_chunks))\n",
        "        encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(\n",
        "                input_ids=encoding.input_ids,\n",
        "                attention_mask=encoding.attention_mask,\n",
        "                generation_config=generation_config,\n",
        "                num_beams=5,  # Use beam search for better results\n",
        "                early_stopping=True,  # Stop early if all beams finish\n",
        "            )\n",
        "\n",
        "        # Exclude the prompt tokens from the generated output\n",
        "        generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "        generated_unpreprocessed_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "        match = re.search(r'\\b[1-4]\\b', generated_unpreprocessed_sequence)\n",
        "        answer = match.group(0) if match else \"\"  # first number found or empty string\n",
        "\n",
        "        with open(f\"quiz/runs_basemodel/quiz_answers_{current_time}.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Question: {q['question']}\\nAnswer: {answer}\\nCorrect answer:{q['correct']}\\nGenerated unpreprocessed sequence: {generated_unpreprocessed_sequence}\\n--------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "        results[q['question_id']] = answer\n",
        "\n",
        "        if len(answer) != 1 or answer not in \"1234\":\n",
        "            error += 1\n",
        "        else:  # the format is correct, now check if the answer is correct\n",
        "            if str(q['correct']) == answer:\n",
        "                count += 1\n",
        "        pbar.set_postfix(Corrects=f\"{count}/{len(questions)}\", Errors=error)\n",
        "\n",
        "    print(\"-------------------------\\tFINISHED RUN. Error count: \", error, \"-------------------------\")\n",
        "    return results, count / len(questions) * 100\n",
        "\n",
        "# Example questions\n",
        "# questions = [\n",
        "#     {\"question\": \"\", \"correct\": \"1\", \"question_id\": \"1\"},\n",
        "#     # Add more questions as needed\n",
        "# ]\n",
        "\n",
        "# results, score = answer_questions(questions)\n",
        "# print(f\"Final score: {score}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVJkzl9Y9yw2"
      },
      "source": [
        "## GRADIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0o1IZXq9yw2"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def query_rag_model(user_query):\n",
        "    top_k_chunks = search(user_query, retriever, k=10)\n",
        "    retrieved_chunks = [chunk for chunk in top_k_chunks]\n",
        "    prompt = base_prompt.format(user_query=user_query, chunks_information=\"\\n\".join(retrieved_chunks))\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    generated_tokens = outputs[0][len(encoding.input_ids[0]):]\n",
        "    generated_unpreprocessed_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    return generated_unpreprocessed_sequence\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query_rag_model,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG Model Query Interface\",\n",
        "    description=\"Ask questions to the RAG model and get answers based on the provided PDF context.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "lm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}